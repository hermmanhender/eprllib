{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hermmanhender/natural_ventilation_EP_RLlib/main/Natural_ventilation_EnergyPlus_in_RLlib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlCJnCgOD5sG"
      },
      "source": [
        "# Implementación de EnergyPlus en Ray montado en Google Colab\n",
        "\n",
        "[Abrir en Colab](https://colab.research.google.com/github.com/hermmanhender/natural_ventilation_EP_RLlib/blob/main/Natural_ventilation_EnergyPlus_in_RLlib.ipynb)\n",
        "\n",
        "Una parte importante de los proyectos que involucran al aprendizaje por refuerzos es el ajuste fino de los hiperparámetros. Esto requiere poder de calculo, por lo que se pretende aquí implementar un servidor de Ray en Google Colab con el fin de ejecutar un experimento utilizando Ray Tune y Ray RLlib.\n",
        "\n",
        "La notebook se erganiza de la siguiente manera:\n",
        "\n",
        "1. **Montaje de Google Drive en Colab.** Esto servirá para alojar los datos generados durante el entrenamiento.\n",
        "2. **Instalación de EnergyPlus.** El entorno de aprendizaje utiliza el programa [EnergyPlus](https://energyplus.net/) para la simulación de edificios.\n",
        "3. **Instalación de librerías.** Se instalan en la máquina virtual las librerías utilizadas para la ejecución del experimento.\n",
        "4. **Definición de funciones.** Se definen las funciones que conforman al entorno de aprendizaje por refuerzos.\n",
        "\n",
        "  4.1. *Comprobación del seriabilidad del entorno.* Para poder distribuir el algoritmo en el servidor es importante que el entorno se pueda seriabilizar. Aquí se comprueba que así sea.\n",
        "\n",
        "5. **Confuguración del algoritmo.** Se configuran los directorios y los hiperparámetros a ajustar en el experimento.\n",
        "6. **Ejecución del experimento.** Se ejecuta la configuración establecida en el punto anterior con Ray Tune. Aquí se configuran los algoritmos de búsqueda y/o de terminación temprana, como así también la cantidad de corridas a realizar y otras relacionadas con el ajuste de los hiperparámetros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhTzgHOIgFSV"
      },
      "source": [
        "## **1**. Drive in Colab mounting\n",
        "\n",
        "It is necesary to have EnergyPlus.sh file for Ubuntu 20.04. This is the operating system in Colab. You can download following [this link](https://github.com/NREL/EnergyPlus/releases/download/v22.1.0/EnergyPlus-22.1.0-ed759b17ee-Linux-Ubuntu20.04-x86_64.sh)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSE8m2seFSWZ",
        "outputId": "4c74b85d-f2aa-4e16-81d1-305aabfb8259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nY4CQqWK9FS"
      },
      "source": [
        "## **2**. Install EnergyPlus in Colab Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgj1EoM4EM65",
        "outputId": "31970578-f961-43ed-c60e-9b2b6f71ce52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EnergyPlus, Copyright (c) 1996-2023, The Board of Trustees of the University of Illinois, The Regents of the University of California, through Lawrence Berkeley National Laboratory (subject to receipt of any required approvals from the U.S. Dept. of Energy), Oak Ridge National Laboratory, managed by UT-Battelle, Alliance for Sustainable Energy, LLC, and other contributors. All rights reserved.\n",
            "\n",
            "NOTICE: This Software was developed under funding from the U.S. Department of Energy and the U.S. Government consequently retains certain rights. As such, the U.S. Government has been granted for itself and others acting on its behalf a paid-up, nonexclusive, irrevocable, worldwide license in the Software to reproduce, distribute copies to the public, prepare derivative works, and perform publicly and display publicly, and to permit others to do so.\n",
            "\n",
            "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
            "\n",
            "(1) Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
            "\n",
            "(2) Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
            "\n",
            "(3) Neither the name of the University of California, Lawrence Berkeley National Laboratory, the University of Illinois, U.S. Dept. of Energy nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
            "\n",
            "(4) Use of EnergyPlus(TM) Name. If Licensee (i) distributes the software in stand-alone form without changes from the version obtained under this License, or (ii) Licensee makes a reference solely to the software portion of its product, Licensee must refer to the software as \"EnergyPlus version X\" software, where \"X\" is the version number Licensee obtained under this License and may not use a different name for the software. Except as specifically required in this Section (4), Licensee shall not use in a company name, a product name, in advertising, publicity, or other promotional activities any name, trade name, trademark, logo, or other designation of \"EnergyPlus\", \"E+\", \"e+\" or confusingly similar designation, without the U.S. Department of Energy's prior written consent.\n",
            "\n",
            "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
            "\n",
            "\n",
            "Do you accept the license? [y/\u001b[1;31mN\u001b[0m]: \n",
            "y\n",
            "EnergyPlus install directory [/usr/local/EnergyPlus-23-2-0]:\n",
            "\n",
            "Symbolic link location (enter \u001b[1;31m\"n\"\u001b[0m for no links) [/usr/local/bin]:\n",
            "\n",
            "Extracting, please wait...\n",
            "Unpacking to directory '/usr/local/EnergyPlus-23-2-0' was successful.\n",
            "Symbolic links were successful.\n",
            "Collecting wurlitzer\n",
            "  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n",
            "Installing collected packages: wurlitzer\n",
            "Successfully installed wurlitzer-3.0.3\n",
            "\n",
            "- Check EnergyPlus Version\n",
            "EnergyPlus, Version 23.2.0-7636e6b3e9\n"
          ]
        }
      ],
      "source": [
        "# install EP to \"/usr/local/EnergyPlus-23-2-0\"\n",
        "!chmod +x //content/drive/MyDrive/ep_drive/EnergyPlus-23.2.0-7636e6b3e9-Linux-Ubuntu20.04-x86_64.sh\n",
        "!sudo /content/drive/MyDrive/ep_drive/EnergyPlus-23.2.0-7636e6b3e9-Linux-Ubuntu20.04-x86_64.sh\n",
        "# to capture C-level stdout/stderr pipes in Python\n",
        "!pip install wurlitzer\n",
        "# check EP\n",
        "print('\\n- Check EnergyPlus Version')\n",
        "!energyplus -version\n",
        "# Add energyplus to PATH\n",
        "import sys\n",
        "sys.path.insert(0, '/usr/local/EnergyPlus-23-2-0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaelbQ7IAJEJ"
      },
      "source": [
        "## **3**. Install all the necesary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOxduxkAN-cK",
        "outputId": "f4408c7d-c46a-482e-9cba-862980b635b7"
      },
      "outputs": [],
      "source": [
        "# Instalación de las librerías\n",
        "!pip install python-multipart\n",
        "!pip install kaleido\n",
        "!pip install ray[all]==2.9.1\n",
        "!pip install gymnasium==0.28.1\n",
        "!pip install bayesian-optimization==1.4.3\n",
        "!pip install tensorflow==2.15.0\n",
        "!pip install torch==2.1.2\n",
        "\n",
        "# setting\n",
        "%load_ext wurlitzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGN5pZecLFPn"
      },
      "source": [
        "## **4**. Definición de funciones\n",
        "\n",
        "Una alternativa es realizar la clonación de un repositorio desde GitHub. Para ello realizamos lo siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# Introduce tu nombre de usuario de GitHub\n",
        "usuario = input('Usuario GitHub: ')\n",
        "# Introduce tu token de acceso personal de GitHub\n",
        "token = getpass('Token de acceso personal de GitHub: ')\n",
        "# URL del repositorio privado\n",
        "url_repositorio = f'https://{usuario}:{token}@github.com/hermmanhender/natural_ventilation_EP_RLlib'\n",
        "\n",
        "# Clonar el repositorio\n",
        "if os.system(f'git clone {url_repositorio}') == 0:\n",
        "    print('Cloning successful.')\n",
        "else:\n",
        "    print('WARNING: Cloning not achieved.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBBEpHSwKgU7"
      },
      "source": [
        "Se establece la configuración del entorno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQKu-sElSPcW"
      },
      "outputs": [],
      "source": [
        "\"\"\"## DEFINE THE EXPERIMENT CONTROLS\n",
        "\"\"\"\n",
        "import gymnasium as gym\n",
        "\n",
        "algorithm = 'DQN'\n",
        "# Define the algorithm to use to train the policy. Options are: PPO, SAC, DQN.\n",
        "tune_runner  = True\n",
        "# Define if the experiment tuning the variables or execute a unique configuration.\n",
        "restore = False\n",
        "# To define if is necesary to restore or not a previous experiment. Is necesary to stablish a 'restore_path'.\n",
        "restore_path = ''\n",
        "# Path to the folder where the experiment is located.\n",
        "\n",
        "env_config = {\n",
        "    'weather_folder': '/content/drive/My Drive/ep_drive/epw',\n",
        "    'output': '/content/drive/My Drive/ep_drive/output',\n",
        "    'epjson_folderpath': '/content/drive/My Drive/ep_drive/epjson',\n",
        "    'epjson_output_folder': '/content/drive/My Drive/ep_drive/models',\n",
        "    # Configure the directories for the experiment.\n",
        "    'ep_terminal_output': False,\n",
        "    # For dubugging is better to print in the terminal the outputs of the EnergyPlus simulation process.\n",
        "    'beta': 0.5,\n",
        "    # This parameter is used to balance between energy and comfort of the inhabitatns. A\n",
        "    # value equal to 0 give a no importance to comfort and a value equal to 1 give no importance\n",
        "    # to energy consume. Mathematically is the reward:\n",
        "    # r = - beta*normaliced_energy - (1-beta)*normalized_comfort\n",
        "    # The range of this value goes from 0.0 to 1.0.,\n",
        "    'is_test': False,\n",
        "    # For evaluation process 'is_test=True' and for trainig False.\n",
        "    'test_init_day': 1,\n",
        "    'action_space': gym.spaces.Discrete(4),\n",
        "    # action space for simple agent case\n",
        "    'observation_space': gym.spaces.Box(float(\"-inf\"), float(\"inf\"), (1465,)),\n",
        "    # observation space for simple agent case\n",
        "\n",
        "    # BUILDING CONFIGURATION\n",
        "    'building_name': 'prot_1',\n",
        "    'volumen': 131.6565,\n",
        "    'window_area_relation_north': 0,\n",
        "    'window_area_relation_west': 0,\n",
        "    'window_area_relation_south': 0.0115243076,\n",
        "    'window_area_relation_east': 0.0276970753,\n",
        "    'episode_len': 365,\n",
        "    'rotation': 0,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFFVrTlKKpFn"
      },
      "source": [
        "### **4.1.** Comprobación del seriabilidad del entorno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yvpdvUZG6lM",
        "outputId": "3693eece-d148-452e-d320-fcd967469aeb"
      },
      "outputs": [],
      "source": [
        "from ray.util import inspect_serializability\n",
        "from env.VENT_ep_gym_env import EnergyPlusEnv_v0\n",
        "\n",
        "# Assume `env` is your environment\n",
        "is_serializable, unserializable_objects = inspect_serializability(EnergyPlusEnv_v0, depth=10)\n",
        "print(f\"Is serializable: {is_serializable}\")\n",
        "if not is_serializable:\n",
        "    print(\"Unserializable objects:\")\n",
        "    for obj in unserializable_objects:\n",
        "        print(obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HxWrmtjVw2s"
      },
      "source": [
        "## **5**. Confuguración del algoritmo\n",
        "\n",
        "Antes de configurar el algoritmo se debe iniciar el servidor de Ray y registrar el enotorno."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "FXmLY2GeWhfK",
        "outputId": "01289263-5cd7-4e3a-bae0-6812d586f9ba"
      },
      "outputs": [],
      "source": [
        "import ray\n",
        "from ray.tune import register_env\n",
        "\n",
        "ray.init()\n",
        "# Inicialiced Ray Server\n",
        "register_env(name=\"EPEnv\", env_creator=lambda args: EnergyPlusEnv_v0(args))\n",
        "# Register the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrcARdTvWKBY"
      },
      "source": [
        "### **5.1.** PPO Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouBr6RkZd5fP"
      },
      "outputs": [],
      "source": [
        "from ray.rllib.algorithms.ppo.ppo import PPOConfig\n",
        "# To config the PPO algorithm.\n",
        "\n",
        "algorithm = 'PPO'\n",
        "# PPO Algorithm Config\n",
        "algo = PPOConfig().training(\n",
        "  # General Algo Configs\n",
        "  gamma=0.72 if not tune_runner else tune.uniform(0.7, 0.99),\n",
        "  # Float specifying the discount factor of the Markov Decision process.\n",
        "  lr=0.04 if not tune_runner else tune.uniform(0.001, 0.1),\n",
        "  # The learning rate (float) or learning rate schedule\n",
        "  #model=,\n",
        "  # Arguments passed into the policy model. See models/catalog.py for a full list of the\n",
        "  # available model options.\n",
        "  train_batch_size=128,# if not tune_runner else tune.choice([128, 256]),\n",
        "  # PPO Configs\n",
        "  lr_schedule=None, # List[List[int | float]] | None = NotProvided,\n",
        "  # Learning rate schedule. In the format of [[timestep, lr-value], [timestep, lr-value], …]\n",
        "  # Intermediary timesteps will be assigned to interpolated learning rate values. A schedule\n",
        "  # should normally start from timestep 0.\n",
        "  use_critic=True, # bool | None = NotProvided,\n",
        "  # Should use a critic as a baseline (otherwise don’t use value baseline; required for using GAE).\n",
        "  use_gae=True, # bool | None = NotProvided,\n",
        "  # If true, use the Generalized Advantage Estimator (GAE) with a value function,\n",
        "  # see https://arxiv.org/pdf/1506.02438.pdf.\n",
        "  lambda_=0.20216 if not tune_runner else tune.uniform(0, 1.0), # float | None = NotProvided,\n",
        "  # The GAE (lambda) parameter.  The generalized advantage estimator for 0 < λ < 1 makes a\n",
        "  # compromise between bias and variance, controlled by parameter λ.\n",
        "  use_kl_loss=True, # bool | None = NotProvided,\n",
        "  # Whether to use the KL-term in the loss function.\n",
        "  kl_coeff=9.9712 if not tune_runner else tune.uniform(0.3, 10.0), # float | None = NotProvided,\n",
        "  # Initial coefficient for KL divergence.\n",
        "  kl_target=0.054921 if not tune_runner else tune.uniform(0.001, 0.1), # float | None = NotProvided,\n",
        "  # Target value for KL divergence.\n",
        "  sgd_minibatch_size=48,# if not tune_runner else tune.choice([48, 128]), # int | None = NotProvided,\n",
        "  # Total SGD batch size across all devices for SGD. This defines the minibatch size\n",
        "  # within each epoch.\n",
        "  num_sgd_iter=6,# if not tune_runner else tune.randint(30, 60), # int | None = NotProvided,\n",
        "  # Number of SGD iterations in each outer loop (i.e., number of epochs to execute per train batch).\n",
        "  shuffle_sequences=True, # bool | None = NotProvided,\n",
        "  # Whether to shuffle sequences in the batch when training (recommended).\n",
        "  vf_loss_coeff=0.38584 if not tune_runner else tune.uniform(0.1, 1.0), # Tune this! float | None = NotProvided,\n",
        "  # Coefficient of the value function loss. IMPORTANT: you must tune this if you set\n",
        "  # vf_share_layers=True inside your model’s config.\n",
        "  entropy_coeff=10.319 if not tune_runner else tune.uniform(0.95, 15.0), # float | None = NotProvided,\n",
        "  # Coefficient of the entropy regularizer.\n",
        "  entropy_coeff_schedule=None, # List[List[int | float]] | None = NotProvided,\n",
        "  # Decay schedule for the entropy regularizer.\n",
        "  clip_param=0.22107 if not tune_runner else tune.uniform(0.1, 0.4), # float | None = NotProvided,\n",
        "  # The PPO clip parameter.\n",
        "  vf_clip_param=39.327 if not tune_runner else tune.uniform(0, 50), # float | None = NotProvided,\n",
        "  # Clip param for the value function. Note that this is sensitive to the scale of the\n",
        "  # rewards. If your expected V is large, increase this.\n",
        "  grad_clip=None, # float | None = NotProvided,\n",
        "  # If specified, clip the global norm of gradients by this amount.\n",
        ").environment(\n",
        "  env=\"EPEnv\",\n",
        "  observation_space=gym.spaces.Box(float(\"-inf\"), float(\"inf\"), (49,)),\n",
        "  action_space=gym.spaces.Discrete(4),\n",
        "  env_config=env_config,\n",
        ").framework(\n",
        "  framework = 'torch',\n",
        ").fault_tolerance(\n",
        "  recreate_failed_workers = True,\n",
        "  restart_failed_sub_environments=False,\n",
        ").rollouts(\n",
        "  num_rollout_workers = 1,# if not tune_runner else tune.grid_search([0, 1, 3]),\n",
        "  create_env_on_local_worker=True,\n",
        "  rollout_fragment_length = 'auto',\n",
        "  enable_connectors = True,\n",
        "  #batch_mode=\"truncate_episodes\",\n",
        "  num_envs_per_worker=1,\n",
        ").experimental(\n",
        "  _enable_new_api_stack = True,\n",
        ").reporting( # multi_agent config va aquí\n",
        "  min_sample_timesteps_per_iteration = 2000,\n",
        ").checkpointing(\n",
        "  export_native_model_files = True,\n",
        ").debugging(\n",
        "  log_level = \"ERROR\",\n",
        "  #seed=7,# if not tune_runner else tune.grid_search([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
        ").resources(\n",
        "  num_gpus = 0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NyWAXdQXmE8"
      },
      "source": [
        "### **5.2.** DQN Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj0j3WmhWYLT"
      },
      "outputs": [],
      "source": [
        "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
        "# To config the DQN algorithm.\n",
        "\n",
        "algo = DQNConfig().training(\n",
        "  # General Algo Configs\n",
        "  gamma = 0.7 if not tune_runner else tune.uniform(0.7, 0.99),\n",
        "  lr = 0.1 if not tune_runner else tune.uniform(0.001, 0.3),\n",
        "  grad_clip = 0.5 if not tune_runner else tune.uniform(0.5, 40.0),\n",
        "  grad_clip_by = 'global_norm',\n",
        "  train_batch_size = 8,# if not tune_runner else tune.choice([4, 8, 128, 256]),\n",
        "  model = {\n",
        "    \"fcnet_hiddens\": [1024,512,512,512],\n",
        "    \"fcnet_activation\": \"relu\", #if not tune_runner else tune.choice(['tanh', 'relu', 'swish', 'linear']),\n",
        "  },\n",
        "  optimizer = {},\n",
        "  # DQN Configs\n",
        "  num_atoms = 40,\n",
        "  v_min = -1,\n",
        "  v_max = 0,\n",
        "  noisy = True,\n",
        "  sigma0 = 0.66 if not tune_runner else tune.uniform(0, 1),\n",
        "  dueling = True,\n",
        "  hiddens = [512],\n",
        "  double_q = True,\n",
        "  n_step = 24,\n",
        "  replay_buffer_config = {\n",
        "    '_enable_replay_buffer_api': True,\n",
        "    'type': 'MultiAgentPrioritizedReplayBuffer',\n",
        "    'capacity': 100000,\n",
        "    'prioritized_replay_alpha': 0.6,\n",
        "    'prioritized_replay_beta': 0.4,\n",
        "    'prioritized_replay_eps': 1e-6,\n",
        "    'replay_sequence_length': 1,\n",
        "  },\n",
        "  categorical_distribution_temperature = 0.5 if not tune_runner else tune.uniform(0, 1),\n",
        ").environment(\n",
        "  env=\"EPEnv\",\n",
        "  env_config=env_config,\n",
        ").framework(\n",
        "  framework = 'torch',\n",
        ").fault_tolerance(\n",
        "  recreate_failed_workers = True,\n",
        "  restart_failed_sub_environments=False,\n",
        ").rollouts(\n",
        "  num_rollout_workers = 1,\n",
        "  create_env_on_local_worker=True,\n",
        "  rollout_fragment_length = 'auto',\n",
        "  enable_connectors = True,\n",
        "  num_envs_per_worker=1,\n",
        ").experimental(\n",
        "  _enable_new_api_stack = False,\n",
        ").reporting( # multi_agent config va aquí\n",
        "  min_sample_timesteps_per_iteration = 1000,\n",
        ").checkpointing(\n",
        "  export_native_model_files = True,\n",
        ").debugging(\n",
        "  log_level = \"ERROR\",\n",
        "  #seed=7,\n",
        ").resources(\n",
        "  num_gpus = 0,\n",
        ")\n",
        "algo.exploration(\n",
        "  exploration_config={\n",
        "    \"type\": \"EpsilonGreedy\",\n",
        "    \"initial_epsilon\": 1.,\n",
        "    \"final_epsilon\": 0.,\n",
        "    \"epsilon_timesteps\": 6*24*365*10,\n",
        "  }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te9UG0-NYNld"
      },
      "source": [
        "### **5.3.** SAC Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5ZcaV8DWa2k"
      },
      "outputs": [],
      "source": [
        "from ray.rllib.algorithms.sac.sac import SACConfig\n",
        "# To config the SAC algorithm.\n",
        "\n",
        "algorithm = 'SAC'\n",
        "algo = SACConfig().training(\n",
        "  # General Algo Configs\n",
        "  gamma = 0.99 if not tune_runner else tune.uniform(0.7, 0.99),\n",
        "  # Float specifying the discount factor of the Markov Decision process.\n",
        "  lr = 0.1 if not tune_runner else tune.uniform(0.001, 0.1),\n",
        "  # The learning rate (float) or learning rate schedule\n",
        "  #grad_clip = None, #float\n",
        "  # If None, no gradient clipping will be applied. Otherwise, depending on the setting of grad_clip_by, the (float)\n",
        "  # value of grad_clip will have the following effect: If grad_clip_by=value: Will clip all computed gradients\n",
        "  # individually inside the interval [-grad_clip, +`grad_clip`]. If grad_clip_by=norm, will compute the L2-norm of\n",
        "  # each weight/bias gradient tensor individually and then clip all gradients such that these L2-norms do not exceed\n",
        "  # grad_clip. The L2-norm of a tensor is computed via: sqrt(SUM(w0^2, w1^2, ..., wn^2)) where w[i] are the elements\n",
        "  # of the tensor (no matter what the shape of this tensor is). If grad_clip_by=global_norm, will compute the square\n",
        "  # of the L2-norm of each weight/bias gradient tensor individually, sum up all these squared L2-norms across all\n",
        "  # given gradient tensors (e.g. the entire module to be updated), square root that overall sum, and then clip all\n",
        "  # gradients such that this global L2-norm does not exceed the given value. The global L2-norm over a list of tensors\n",
        "  # (e.g. W and V) is computed via: sqrt[SUM(w0^2, w1^2, ..., wn^2) + SUM(v0^2, v1^2, ..., vm^2)], where w[i] and v[j]\n",
        "  # are the elements of the tensors W and V (no matter what the shapes of these tensors are).\n",
        "  #grad_clip_by = 'global_norm', #str\n",
        "  # See grad_clip for the effect of this setting on gradient clipping. Allowed values are value, norm, and global_norm.\n",
        "  #train_batch_size = 128, # if not tune_runner else tune.randint(128, 257),\n",
        "  #  Training batch size, if applicable.\n",
        "  model = {\n",
        "    \"fcnet_hiddens\": [256],\n",
        "    \"fcnet_activation\": \"relu\",\n",
        "  },\n",
        "  # Arguments passed into the policy model. See models/catalog.py for a full list of the\n",
        "  # available model options. TODO: Provide ModelConfig objects instead of dicts\n",
        "  #optimizer = None, #dict\n",
        "  # Arguments to pass to the policy optimizer. This setting is not used when _enable_new_api_stack=True.\n",
        "  #max_requests_in_flight_per_sampler_worker = None, #int\n",
        "  # Max number of inflight requests to each sampling worker. See the FaultTolerantActorManager class for more details.\n",
        "  # Tuning these values is important when running experimens with large sample batches, where there is the risk that\n",
        "  # the object store may fill up, causing spilling of objects to disk. This can cause any asynchronous requests to\n",
        "  # become very slow, making your experiment run slow as well. You can inspect the object store during your experiment\n",
        "  # via a call to ray memory on your headnode, and by using the ray dashboard. If you’re seeing that the object store\n",
        "  # is filling up, turn down the number of remote requests in flight, or enable compression in your experiment of\n",
        "  # timesteps.\n",
        "  #learner_class = None,\n",
        "  # The Learner class to use for (distributed) updating of the RLModule. Only used when _enable_new_api_stack=True.\n",
        "\n",
        "  # SAC Configs\n",
        "  twin_q = True, #bool\n",
        "  # Use two Q-networks (instead of one) for action-value estimation. Note: Each Q-network will have its own target network.\n",
        "  #q_model_config = #~typing.Dict[str, ~typing.Any]\n",
        "  # Model configs for the Q network(s). These will override MODEL_DEFAULTS. This is treated just as the top-level model\n",
        "  # dict in setting up the Q-network(s) (2 if twin_q=True). That means, you can do for different observation spaces:\n",
        "  # obs=Box(1D) -> Tuple(Box(1D) + Action) -> concat -> post_fcnet obs=Box(3D) -> Tuple(Box(3D) + Action) ->\n",
        "  # vision-net -> concat w/ action -> post_fcnet obs=Tuple(Box(1D), Box(3D)) -> Tuple(Box(1D), Box(3D), Action) ->\n",
        "  # vision-net -> concat w/ Box(1D) and action -> post_fcnet You can also have SAC use your custom_model as Q-model(s),\n",
        "  # by simply specifying the custom_model sub-key in below dict (just like you would do in the top-level model dict.\n",
        "  #policy_model_config = #~typing.Dict[str, ~typing.Any]\n",
        "  # Model options for the policy function (see q_model_config above for details). The difference to q_model_config above\n",
        "  # is that no action concat’ing is performed before the post_fcnet stack.\n",
        "  tau = 1.0, #float\n",
        "  # Update the target by au * policy + (1- au) * target_policy.\n",
        "  initial_alpha = 0.5, #float\n",
        "  # Initial value to use for the entropy weight alpha.\n",
        "  target_entropy = 'auto', #str | float\n",
        "  # Target entropy lower bound. If “auto”, will be set to -|A| (e.g. -2.0 for Discrete(2), -3.0 for Box(shape=(3,))). This\n",
        "  # is the inverse of reward scale, and will be optimized automatically.\n",
        "  n_step = 10, # if not tune_runner else tune.randint(1, 11), #int\n",
        "  # N-step target updates. If >1, sars’ tuples in trajectories will be postprocessed to become\n",
        "  # sa[discounted sum of R][s t+n] tuples.\n",
        "  store_buffer_in_checkpoints = True, #bool\n",
        "  # Set this to True, if you want the contents of your buffer(s) to be stored in any saved checkpoints as well. Warnings\n",
        "  # will be created if: - This is True AND restoring from a checkpoint that contains no buffer data. - This is\n",
        "  # False AND restoring from a checkpoint that does contain buffer data.\n",
        "  replay_buffer_config = {\n",
        "    '_enable_replay_buffer_api': True,\n",
        "    'type': 'MultiAgentPrioritizedReplayBuffer',\n",
        "    'capacity': 50000,\n",
        "    'prioritized_replay_alpha': 0.6,\n",
        "    'prioritized_replay_beta': 0.4,\n",
        "    'prioritized_replay_eps': 1e-6,\n",
        "    'replay_sequence_length': 1,\n",
        "  },\n",
        "  # Replay buffer config. Examples: { “_enable_replay_buffer_api”: True, “type”: “MultiAgentReplayBuffer”,\n",
        "  # “capacity”: 50000, “replay_batch_size”: 32, “replay_sequence_length”: 1, } - OR - { “_enable_replay_buffer_api”: True,\n",
        "  # “type”: “MultiAgentPrioritizedReplayBuffer”, “capacity”: 50000, “prioritized_replay_alpha”: 0.6,\n",
        "  # “prioritized_replay_beta”: 0.4, “prioritized_replay_eps”: 1e-6, “replay_sequence_length”: 1, } - Where -\n",
        "  # prioritized_replay_alpha: Alpha parameter controls the degree of prioritization in the buffer. In other words, when\n",
        "  # a buffer sample has a higher temporal-difference error, with how much more probability should it drawn to use\n",
        "  # to update the parametrized Q-network. 0.0 corresponds to uniform probability. Setting much above 1.0 may quickly\n",
        "  # result as the sampling distribution could become heavily “pointy” with low entropy. prioritized_replay_beta: Beta\n",
        "  # parameter controls the degree of importance sampling which suppresses the influence of gradient updates from\n",
        "  # samples that have higher probability of being sampled via alpha parameter and the temporal-difference error.\n",
        "  # prioritized_replay_eps: Epsilon parameter sets the baseline probability for sampling so that when the\n",
        "  # temporal-difference error of a sample is zero, there is still a chance of drawing the sample.\n",
        "  #training_intensity = #float\n",
        "  # The intensity with which to update the model (vs collecting samples from the env). If None, uses “natural” values\n",
        "  # of: train_batch_size / (rollout_fragment_length x num_workers x num_envs_per_worker). If not None, will make sure\n",
        "  # that the ratio between timesteps inserted into and sampled from th buffer matches the given values. Example:\n",
        "  # training_intensity=1000.0 train_batch_size=250 rollout_fragment_length=1 num_workers=1 (or 0) num_envs_per_worker=1 ->\n",
        "  # natural value = 250 / 1 = 250.0 -> will make sure that replay+train op will be executed 4x asoften as rollout+insert\n",
        "  # op (4 * 250 = 1000). See: rllib/algorithms/dqn/dqn.py::calculate_rr_weights for further details.\n",
        "  clip_actions = True, #bool\n",
        "  # Whether to clip actions. If actions are already normalized, this should be set to False.\n",
        "  #grad_clip = #float\n",
        "  # If not None, clip gradients during optimization at this value.\n",
        "  optimization_config = { #~typing.Dict[str, ~typing.Any]\n",
        "    'actor_learning_rate': 0.005,\n",
        "    'critic_learning_rate': 0.005,\n",
        "    'entropy_learning_rate': 0.0001,\n",
        "  },\n",
        "  # Config dict for optimization. Set the supported keys actor_learning_rate, critic_learning_rate, and\n",
        "  # entropy_learning_rate in here.\n",
        "  target_network_update_freq = 144, #int\n",
        "  # Update the target network every target_network_update_freq steps.\n",
        "  #_deterministic_loss = #bool\n",
        "  # Whether the loss should be calculated deterministically (w/o the stochastic action sampling step). True only useful\n",
        "  # for continuous actions and for debugging.\n",
        "  #_use_beta_distribution = #bool\n",
        "  # Use a Beta-distribution instead of a SquashedGaussian for bounded, continuous action spaces (not recommended; for\n",
        "  # debugging only).\n",
        ").environment(\n",
        "  env=\"EPEnv\",\n",
        "  observation_space=gym.spaces.Box(float(\"-inf\"), float(\"inf\"), (49,)),\n",
        "  action_space=gym.spaces.Discrete(4),\n",
        "  env_config=env_config,\n",
        ").framework(\n",
        "  framework = 'torch',\n",
        ").fault_tolerance(\n",
        "  recreate_failed_workers = True,\n",
        "  restart_failed_sub_environments=False,\n",
        ").rollouts(\n",
        "  num_rollout_workers = 1,# if not tune_runner else tune.grid_search([0, 1, 3]),\n",
        "  create_env_on_local_worker=True,\n",
        "  rollout_fragment_length = 'auto',\n",
        "  enable_connectors = True,\n",
        "  #batch_mode=\"truncate_episodes\",\n",
        "  num_envs_per_worker=1,\n",
        ").experimental(\n",
        "  _enable_new_api_stack = True,\n",
        ").reporting( # multi_agent config va aquí\n",
        "  min_sample_timesteps_per_iteration = 2000,\n",
        ").checkpointing(\n",
        "  export_native_model_files = True,\n",
        ").debugging(\n",
        "  log_level = \"ERROR\",\n",
        ").resources(\n",
        "  num_gpus = 0,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W79l5StOYnMb"
      },
      "source": [
        "## **6.** Ejecución del experimento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ray import air, tune\n",
        "# To configurate the execution of the experiment\n",
        "\n",
        "def trial_str_creator(trial):\n",
        "    return \"{}_{}_{}_REP\".format(trial.trainable_name, trial.trial_id, tune.search.repeater.TRIAL_INDEX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Svox8sD4Yls-"
      },
      "outputs": [],
      "source": [
        "from ray.tune.schedulers import ASHAScheduler\n",
        "# Early stop to tune the hyperparameters\n",
        "from ray.tune.search.bayesopt import BayesOptSearch\n",
        "# Search algorithm to tune the hyperparameters\n",
        "from ray.tune.search import Repeater\n",
        "# Tool to evaluate multiples seeds in a configuration of hyperparameters\n",
        "\n",
        "if not restore:\n",
        "    tune.Tuner(\n",
        "        algorithm,\n",
        "        tune_config=tune.TuneConfig(\n",
        "            mode=\"max\",\n",
        "            metric=\"episode_reward_mean\",\n",
        "            num_samples=1000,\n",
        "            # This is necesary to iterative execute the search_alg to improve the hyperparameters\n",
        "            reuse_actors=False,\n",
        "            trial_name_creator=trial_str_creator,\n",
        "            trial_dirname_creator=trial_str_creator,\n",
        "\n",
        "            #search_alg = Repeater(BayesOptSearch(),repeat=10),\n",
        "            search_alg = BayesOptSearch(),\n",
        "            # Search algorithm\n",
        "\n",
        "            #scheduler = ASHAScheduler(time_attr = 'timesteps_total', max_t=6*24*365*3, grace_period=6*24*365),\n",
        "            # Scheduler algorithm\n",
        "\n",
        "        ),\n",
        "        run_config=air.RunConfig(\n",
        "            name='BOS_VN_P1_'+str(env_config['beta'])+'_'+str(algorithm),\n",
        "            stop={\"timesteps_total\": 6*24*365*20},\n",
        "            log_to_file=True,\n",
        "\n",
        "            checkpoint_config=air.CheckpointConfig(\n",
        "                checkpoint_at_end = True,\n",
        "                checkpoint_frequency = 40,\n",
        "                #num_to_keep = 20\n",
        "            ),\n",
        "            failure_config=air.FailureConfig(\n",
        "                max_failures=100\n",
        "                # Tries to recover a run up to this many times.\n",
        "            ),\n",
        "        ),\n",
        "        param_space=algo.to_dict(),\n",
        "    ).fit()\n",
        "\n",
        "else:\n",
        "    tune.Tuner.restore(\n",
        "        path=restore_path,\n",
        "        trainable = algorithm,\n",
        "        resume_errored=True\n",
        "    )\n",
        "\n",
        "\"\"\"## END EXPERIMENT AND SHUTDOWN RAY SERVE\n",
        "\"\"\"\n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYDclTeOj6Q8"
      },
      "outputs": [],
      "source": [
        "ray.shutdown()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "YGN5pZecLFPn",
        "IvJvHYM9LL_T",
        "EXNK9i1HUF3s",
        "5GNTE9IeVYni",
        "lrcARdTvWKBY",
        "1NyWAXdQXmE8",
        "Te9UG0-NYNld"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

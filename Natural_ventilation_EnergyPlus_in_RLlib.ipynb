{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementación de EnergyPlus en Ray montado en Google Colab\n",
        "\n",
        "Una parte importante de los proyectos que involucran al aprendizaje por refuerzos es el ajuste fino de los hiperparámetros. Esto requiere poder de calculo, por lo que se pretende aquí implementar un servidor de Ray en Google Colab con el fin de ejecutar un experimento utilizando Ray Tune y Ray RLlib.\n",
        "\n",
        "La notebook se erganiza de la siguiente manera:\n",
        "\n",
        "1. **Montaje de Google Drive en Colab.** Esto servirá para alojar los datos generados durante el entrenamiento.\n",
        "2. **Instalación de EnergyPlus.** El entorno de aprendizaje utiliza el programa [EnergyPlus](https://energyplus.net/) para la simulación de edificios.\n",
        "3. **Instalación de librerías.** Se instalan en la máquina virtual las librerías utilizadas para la ejecución del experimento.\n",
        "4. **Definición de funciones.** Se definen las funciones que conforman al entorno de aprendizaje por refuerzos.\n",
        "\n",
        "  4.1. *Comprobación del seriabilidad del entorno.* Para poder distribuir el algoritmo en el servidor es importante que el entorno se pueda seriabilizar. Aquí se comprueba que así sea.\n",
        "\n",
        "5. **Confuguración del algoritmo.** Se configuran los directorios y los hiperparámetros a ajustar en el experimento.\n",
        "6. **Ejecución del experimento.** Se ejecuta la configuración establecida en el punto anterior con Ray Tune. Aquí se configuran los algoritmos de búsqueda y/o de terminación temprana, como así también la cantidad de corridas a realizar y otras relacionadas con el ajuste de los hiperparámetros."
      ],
      "metadata": {
        "id": "mlCJnCgOD5sG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhTzgHOIgFSV"
      },
      "source": [
        "## **1**. Drive in Colab mounting\n",
        "\n",
        "It is necesary to have EnergyPlus.sh file for Ubuntu 20.04. This is the operating system in Colab. You can download following [this link](https://github.com/NREL/EnergyPlus/releases/download/v22.1.0/EnergyPlus-22.1.0-ed759b17ee-Linux-Ubuntu20.04-x86_64.sh)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSE8m2seFSWZ",
        "outputId": "4c74b85d-f2aa-4e16-81d1-305aabfb8259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nY4CQqWK9FS"
      },
      "source": [
        "## **2**. Install EnergyPlus in Colab Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgj1EoM4EM65",
        "outputId": "31970578-f961-43ed-c60e-9b2b6f71ce52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EnergyPlus, Copyright (c) 1996-2023, The Board of Trustees of the University of Illinois, The Regents of the University of California, through Lawrence Berkeley National Laboratory (subject to receipt of any required approvals from the U.S. Dept. of Energy), Oak Ridge National Laboratory, managed by UT-Battelle, Alliance for Sustainable Energy, LLC, and other contributors. All rights reserved.\n",
            "\n",
            "NOTICE: This Software was developed under funding from the U.S. Department of Energy and the U.S. Government consequently retains certain rights. As such, the U.S. Government has been granted for itself and others acting on its behalf a paid-up, nonexclusive, irrevocable, worldwide license in the Software to reproduce, distribute copies to the public, prepare derivative works, and perform publicly and display publicly, and to permit others to do so.\n",
            "\n",
            "Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
            "\n",
            "(1) Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
            "\n",
            "(2) Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
            "\n",
            "(3) Neither the name of the University of California, Lawrence Berkeley National Laboratory, the University of Illinois, U.S. Dept. of Energy nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n",
            "\n",
            "(4) Use of EnergyPlus(TM) Name. If Licensee (i) distributes the software in stand-alone form without changes from the version obtained under this License, or (ii) Licensee makes a reference solely to the software portion of its product, Licensee must refer to the software as \"EnergyPlus version X\" software, where \"X\" is the version number Licensee obtained under this License and may not use a different name for the software. Except as specifically required in this Section (4), Licensee shall not use in a company name, a product name, in advertising, publicity, or other promotional activities any name, trade name, trademark, logo, or other designation of \"EnergyPlus\", \"E+\", \"e+\" or confusingly similar designation, without the U.S. Department of Energy's prior written consent.\n",
            "\n",
            "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
            "\n",
            "\n",
            "Do you accept the license? [y/\u001b[1;31mN\u001b[0m]: \n",
            "y\n",
            "EnergyPlus install directory [/usr/local/EnergyPlus-23-2-0]:\n",
            "\n",
            "Symbolic link location (enter \u001b[1;31m\"n\"\u001b[0m for no links) [/usr/local/bin]:\n",
            "\n",
            "Extracting, please wait...\n",
            "Unpacking to directory '/usr/local/EnergyPlus-23-2-0' was successful.\n",
            "Symbolic links were successful.\n",
            "Collecting wurlitzer\n",
            "  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n",
            "Installing collected packages: wurlitzer\n",
            "Successfully installed wurlitzer-3.0.3\n",
            "\n",
            "- Check EnergyPlus Version\n",
            "EnergyPlus, Version 23.2.0-7636e6b3e9\n"
          ]
        }
      ],
      "source": [
        "# install EP to \"/usr/local/EnergyPlus-23-2-0\"\n",
        "!chmod +x //content/drive/MyDrive/ep_drive/EnergyPlus-23.2.0-7636e6b3e9-Linux-Ubuntu20.04-x86_64.sh\n",
        "!sudo /content/drive/MyDrive/ep_drive/EnergyPlus-23.2.0-7636e6b3e9-Linux-Ubuntu20.04-x86_64.sh\n",
        "# to capture C-level stdout/stderr pipes in Python\n",
        "!pip install wurlitzer\n",
        "# check EP\n",
        "print('\\n- Check EnergyPlus Version')\n",
        "!energyplus -version\n",
        "# Add energyplus to PATH\n",
        "import sys\n",
        "sys.path.insert(0, '/usr/local/EnergyPlus-23-2-0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaelbQ7IAJEJ"
      },
      "source": [
        "## **3**. Install all the necesary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOxduxkAN-cK",
        "outputId": "f4408c7d-c46a-482e-9cba-862980b635b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaleido\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed kaleido-0.2.1\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.9\n",
            "Requirement already satisfied: ray[all]==2.9.1 in /usr/local/lib/python3.10/dist-packages (2.9.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (1.0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (23.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (2.31.0)\n",
            "Requirement already satisfied: colorful in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (0.5.6)\n",
            "Requirement already satisfied: watchfiles in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (0.21.0)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (4.3.3)\n",
            "Requirement already satisfied: starlette in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (0.36.3)\n",
            "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (0.3.14)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (0.9.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (0.19.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (2023.6.0)\n",
            "Requirement already satisfied: aiorwlock in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (1.4.0)\n",
            "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (0.7.0)\n",
            "Requirement already satisfied: ray-cpp==2.9.1 in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (2.9.1)\n",
            "Requirement already satisfied: opencensus in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (0.11.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (1.11.4)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (1.60.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (1.5.3)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (3.9.3)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (0.19.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (1.22.0)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (6.4.0)\n",
            "Requirement already satisfied: uvicorn[standard] in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (0.27.1)\n",
            "Requirement already satisfied: gpustat>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (1.1.1)\n",
            "Requirement already satisfied: opentelemetry-api in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (1.22.0)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3 in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (1.25.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (0.1.8)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (0.109.2)\n",
            "Requirement already satisfied: opentelemetry-sdk in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (1.22.0)\n",
            "Requirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (20.25.0)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (2.6.2.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (13.7.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (10.0.1)\n",
            "Requirement already satisfied: gymnasium==0.28.1 in /usr/local/lib/python3.10/dist-packages (from ray[all]==2.9.1) (0.28.1)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[all]==2.9.1) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[all]==2.9.1) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[all]==2.9.1) (4.9.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[all]==2.9.1) (0.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[all]==2.9.1) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[all]==2.9.1) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[all]==2.9.1) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp>=3.7->ray[all]==2.9.1) (4.0.3)\n",
            "Requirement already satisfied: nvidia-ml-py>=11.450.129 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[all]==2.9.1) (12.535.133)\n",
            "Requirement already satisfied: psutil>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[all]==2.9.1) (5.9.5)\n",
            "Requirement already satisfied: blessed>=1.17.1 in /usr/local/lib/python3.10/dist-packages (from gpustat>=1.0.0->ray[all]==2.9.1) (1.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[all]==2.9.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[all]==2.9.1) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[all]==2.9.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[all]==2.9.1) (2.16.2)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[all]==2.9.1) (0.3.8)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[all]==2.9.1) (4.2.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->ray[all]==2.9.1) (3.7.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[all]==2.9.1) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[all]==2.9.1) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[all]==2.9.1) (0.17.1)\n",
            "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[all]==2.9.1) (0.1.3)\n",
            "Requirement already satisfied: six~=1.16 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[all]==2.9.1) (1.16.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opencensus->ray[all]==2.9.1) (2.11.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api->ray[all]==2.9.1) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api->ray[all]==2.9.1) (6.11.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.22.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp->ray[all]==2.9.1) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.22.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp->ray[all]==2.9.1) (1.22.0)\n",
            "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.22.0->opentelemetry-exporter-otlp->ray[all]==2.9.1) (2.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.22.0->opentelemetry-exporter-otlp->ray[all]==2.9.1) (1.62.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.22.0->opentelemetry-exporter-otlp->ray[all]==2.9.1) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.22.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.22.0->opentelemetry-exporter-otlp->ray[all]==2.9.1) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk->ray[all]==2.9.1) (0.43b0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[all]==2.9.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[all]==2.9.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[all]==2.9.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[all]==2.9.1) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[all]==2.9.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[all]==2.9.1) (2.16.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[all]==2.9.1) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[all]==2.9.1) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[all]==2.9.1) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[all]==2.9.1) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[all]==2.9.1) (1.5.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->ray[all]==2.9.1) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->ray[all]==2.9.1) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->ray[all]==2.9.1) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->ray[all]==2.9.1) (0.19.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->ray[all]==2.9.1) (12.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->ray[all]==2.9.1) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->ray[all]==2.9.1) (1.2.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[all]==2.9.1) (0.2.13)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api->ray[all]==2.9.1) (1.14.1)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[all]==2.9.1) (2.27.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api->ray[all]==2.9.1) (3.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->ray[all]==2.9.1) (0.1.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[all]==2.9.1) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[all]==2.9.1) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[all]==2.9.1) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[all]==2.9.1) (0.5.1)\n",
            "Requirement already satisfied: gymnasium==0.28.1 in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1) (1.25.2)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1) (4.9.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1) (0.0.4)\n",
            "Requirement already satisfied: bayesian-optimization==1.4.3 in /usr/local/lib/python3.10/dist-packages (1.4.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization==1.4.3) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization==1.4.3) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization==1.4.3) (1.2.2)\n",
            "Requirement already satisfied: colorama>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization==1.4.3) (0.4.6)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization==1.4.3) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization==1.4.3) (3.2.0)\n",
            "Requirement already satisfied: tensorflow==2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.9.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.60.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Requirement already satisfied: torch==2.1.2 in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.2) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2) (12.3.101)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.2) (1.3.0)\n",
            "The wurlitzer extension is already loaded. To reload it, use:\n",
            "  %reload_ext wurlitzer\n"
          ]
        }
      ],
      "source": [
        "# Instalación de las librerías\n",
        "!pip install python-multipart\n",
        "!pip install kaleido\n",
        "!pip install ray[all]==2.9.1\n",
        "!pip install gymnasium==0.28.1\n",
        "!pip install bayesian-optimization==1.4.3\n",
        "!pip install tensorflow==2.15.0\n",
        "!pip install torch==2.1.2\n",
        "\n",
        "# setting\n",
        "%load_ext wurlitzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGN5pZecLFPn"
      },
      "source": [
        "## **4**. Definición de funciones\n",
        "\n",
        "Una alternativa es realizar la clonación de un repositorio desde GitHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Afvf9He_dNDr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import gymnasium as gym\n",
        "# Used to configurate the action and observation spaces\n",
        "os.environ['RAY_PICKLE_VERBOSE_DEBUG'] = '2'\n",
        "import ray\n",
        "# To init ray\n",
        "from ray import air, tune\n",
        "# To configurate the execution of the experiment\n",
        "from ray.tune import register_env\n",
        "# To register the custom environment. RLlib is not compatible with conventional Gym register of\n",
        "# custom environments.\n",
        "from ray.rllib.algorithms.ppo.ppo import PPOConfig\n",
        "# To config the PPO algorithm.\n",
        "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
        "# To config the DQN algorithm.\n",
        "from ray.rllib.algorithms.sac.sac import SACConfig\n",
        "# To config the SAC algorithm.\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "# Early stop to tune the hyperparameters\n",
        "from ray.tune.search.bayesopt import BayesOptSearch\n",
        "# Search algorithm to tune the hyperparameters\n",
        "from ray.tune.search import Repeater\n",
        "# Tool to evaluate multiples seeds in a configuration of hyperparameters\n",
        "#from env.VENT_ep_gym_env import EnergyPlusEnv_v0\n",
        "# The EnergyPlus Environment configuration. There is defined the reward function\n",
        "# and also is define the flux of execution of the MDP.\n",
        "\n",
        "import sys\n",
        "import threading\n",
        "from time import sleep\n",
        "from typing import Any, Dict, List, Optional\n",
        "from queue import Empty, Full, Queue\n",
        "# Used to separate the execution in two threads and comunicate EnergyPlus with this environment.\n",
        "\n",
        "\n",
        "os_platform = sys.platform\n",
        "if os_platform == \"linux\":\n",
        "    sys.path.insert(0, '/usr/local/EnergyPlus-23-2-0')\n",
        "else:\n",
        "    sys.path.insert(0, 'C:/EnergyPlusV23-2-0')\n",
        "\n",
        "from pyenergyplus.api import EnergyPlusAPI\n",
        "api = EnergyPlusAPI()\n",
        "\n",
        "\"\"\"Action spaces for diferent agents that operate devices in a centralized way.\n",
        "\"\"\"\n",
        "\n",
        "def natural_ventilation_action(central_action: int):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        central_action (int): _description_\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "    action_space = [\n",
        "        [0,0],\n",
        "        [0,1],\n",
        "        [1,0],\n",
        "        [1,1]\n",
        "    ]\n",
        "    return action_space[central_action]\n",
        "\n",
        "def natural_ventilation_central_action(action1: int, action2: int):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        action1 (int): _description_\n",
        "        action2 (int): _description_\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "    action_space = [\n",
        "        [0,0],\n",
        "        [0,1],\n",
        "        [1,0],\n",
        "        [1,1]\n",
        "    ]\n",
        "    index = 0\n",
        "    for a in action_space:\n",
        "        if a == [action1, action2]:\n",
        "            central_action = index\n",
        "            break\n",
        "        else:\n",
        "            index += 1\n",
        "\n",
        "    return central_action\n",
        "\n",
        "class TwoWindowsCentralizedControl:\n",
        "    def __init__(self):\n",
        "        self.action_space = [\n",
        "            [0,0],\n",
        "            [0,1],\n",
        "            [1,0],\n",
        "            [1,1]\n",
        "        ]\n",
        "\n",
        "    def natural_ventilation_action(self, central_action: int):\n",
        "        \"\"\"_summary_\n",
        "\n",
        "        Args:\n",
        "            central_action (int): _description_\n",
        "\n",
        "        Returns:\n",
        "            _type_: _description_\n",
        "        \"\"\"\n",
        "        return self.action_space[central_action]\n",
        "\n",
        "    def natural_ventilation_central_action(self, action1: int, action2: int):\n",
        "        \"\"\"_summary_\n",
        "\n",
        "        Args:\n",
        "            action1 (int): _description_\n",
        "            action2 (int): _description_\n",
        "\n",
        "        Returns:\n",
        "            _type_: _description_\n",
        "        \"\"\"\n",
        "        index = 0\n",
        "        for a in self.action_space:\n",
        "            if a == [action1, action2]:\n",
        "                central_action = index\n",
        "                break\n",
        "            else:\n",
        "                index += 1\n",
        "\n",
        "        return central_action\n",
        "\n",
        "\"\"\"Utilities that involve the weather.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "from pandas.core.frame import DataFrame\n",
        "import numpy as np\n",
        "\n",
        "def weather_file(env_config: dict, weather_choice:int = np.random.randint(0,3)):\n",
        "    \"\"\"This method select a random or specific weather file path and the respectives latitude, longitude, and altitude for\n",
        "    the weather path training options or the path to be use for evaluation.\n",
        "\n",
        "    Args:\n",
        "        env_config (dict): Environment configuration with the 'weather_folder' path and the specification of 'is_test' condition.\n",
        "        weather_choice (int, optional): This option provide to select only one weather file for training. Defaults to np.random.randint(0,3).\n",
        "\n",
        "    Returns:\n",
        "        tuple[str, float, float, int]: Return a tuple with the epw path and the respective values for latitude, longitude, and altitude.\n",
        "    \"\"\"\n",
        "    folder_path = env_config['weather_folder']\n",
        "    if not env_config['is_test']:\n",
        "        weather_path = [\n",
        "            ['GEF_Lujan_de_cuyo-hour-H1',-32.985,-68.93,1043],\n",
        "            ['GEF_Lujan_de_cuyo-hour-H2',-32.985,-68.93,1043],\n",
        "            ['GEF_Lujan_de_cuyo-hour-H3',-32.985,-68.93,1043],\n",
        "        ]\n",
        "        latitud = weather_path[weather_choice][1]\n",
        "        longitud = weather_path[weather_choice][2]\n",
        "        altitud = weather_path[weather_choice][3]\n",
        "        return folder_path+'/'+weather_path[weather_choice][0]+'.epw', latitud, longitud, altitud\n",
        "    else:\n",
        "        return folder_path+'/GEF_Lujan_de_cuyo-hour-H4.epw', -32.985,-68.93,1043\n",
        "\n",
        "class Probabilities:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env_config:dict\n",
        "    ):\n",
        "        \"\"\"This class provide methods to calculate the weather probabilities during training based on the weather file 'epw'.\n",
        "\n",
        "        Args:\n",
        "            env_config (dict): Environment configuration with the 'epw' path element.\n",
        "\n",
        "        Example:\n",
        "        ```\n",
        "        >>> from tools.weather_utils import Probabilities, weather_file\n",
        "        >>> env_config={\n",
        "                'weather_folder': 'C:/Users/grhen/Documents/GitHub/natural_ventilation_EP_RLlib/epw/GEF',\n",
        "                'is_test': False,\n",
        "            }\n",
        "        >>> env_config['epw'], _, _, _ = weather_file(env_config)\n",
        "        >>> prob = Probabilities(env_config)\n",
        "        >>> julian_day = 215\n",
        "        >>> predictions = prob.ten_days_predictions(julian_day)\n",
        "        ```\n",
        "        \"\"\"\n",
        "        self.env_config = env_config\n",
        "\n",
        "        with open(self.env_config[\"epw\"]) as file:\n",
        "            self.weather_file: DataFrame = pd.read_csv(\n",
        "                file,\n",
        "                header = None,\n",
        "                skiprows = 8\n",
        "            )\n",
        "            # Reading the weather epw file.\n",
        "        self.ten_rows_added = False\n",
        "        # Flag to be sure about the run of the next line.\n",
        "        self.complement_10_days()\n",
        "\n",
        "    def complement_10_days(self):\n",
        "        \"\"\"This method add rows to complement the predictions of the entire year of then days after the December 31th using the first\n",
        "        ten days of the year. For that, 240 rows are added because each day has 24 hours.\n",
        "        \"\"\"\n",
        "        primeras_10_filas = self.weather_file.head(240)\n",
        "        # Obtain the first 240 rows of the weather file.\n",
        "        self.weather_file = pd.concat([self.weather_file, primeras_10_filas], ignore_index=True)\n",
        "        # Add the rows to the same weather file.\n",
        "        self.ten_rows_added = True\n",
        "        # Put this flag in True mode.\n",
        "\n",
        "\n",
        "    # Paso 1: Filtrar los datos para el día juliano dado y los próximos 9 días\n",
        "    def julian_day_filter(self, dia_juliano: int):\n",
        "        \"\"\"This method implement a filter of the weather data based on the julian day `n` and create a NDarray with booleans with\n",
        "        True values in the data filtered from `[n, n+10]` bouth inclusive.\n",
        "\n",
        "        Args:\n",
        "            dia_juliano (int): First julian day of the range filtered.\n",
        "\n",
        "        Returns:\n",
        "            np_ndarray_bool\n",
        "        \"\"\"\n",
        "        if self.ten_rows_added:\n",
        "            # The julian day of each row is calculated for a extended list with 10 days more.\n",
        "            dias_julianos = ((self.weather_file.index % 9240) // 24 + 1)\n",
        "        else:\n",
        "            # The julian day of each row is calculated for a not extended.\n",
        "            dias_julianos = (self.weather_file.index % 8760) // 24 + 1\n",
        "        # Check if the Julian day is within the desired range and return\n",
        "        return dias_julianos.isin(range(dia_juliano, dia_juliano + 10))\n",
        "\n",
        "    def ten_days_predictions(self, julian_day: int):\n",
        "        \"\"\"This method calculate the probabilies of six variables list bellow with a normal probability based on the desviation\n",
        "        of the variable.\n",
        "\n",
        "            Dry Bulb Temperature in °C with desviation of 1 °C,\n",
        "            Relative Humidity in % with desviation of 10%,\n",
        "            Wind Direction in degree with desviation of 20°,\n",
        "            Wind Speed in m/s with desviation of 0.5 m/s,\n",
        "            Total Sky in % Cover with desviation of 10%,\n",
        "            Liquid Precipitation Depth in mm with desviation of 0.2 mm.\n",
        "\n",
        "        Args:\n",
        "            julian_day (int): First julian day of the range of ten days predictions.\n",
        "\n",
        "        Returns:\n",
        "            NDArray: Array with the ten days predictions. The size of the array is a sigle shape with 1440 values.\n",
        "        \"\"\"\n",
        "        interest_variables = [6, 8, 20, 21, 22, 33]\n",
        "        # This corresponds with the epw file order.\n",
        "        filtered_data: DataFrame = self.weather_file[self.julian_day_filter(julian_day)][interest_variables]\n",
        "        # Filter the data whith the julian day of interes and ten days ahead.\n",
        "        data_list: list = filtered_data.values.tolist()\n",
        "        # Transform the DataFrame into a list. This list contain a list for each hour, but as an observation of a single shape in\n",
        "        # the RLlib configuration, the list is transform into a new one with only a shape.\n",
        "        single_shape_list = []\n",
        "        for e in range(len(data_list)):\n",
        "            for v in data_list[e]:\n",
        "                single_shape_list.append(v)\n",
        "                # append each value of each day and hour in a consecutive way in the empty list.\n",
        "        desviation = [1, 10, 20, 0.5, 10, 0.2]\n",
        "        # Assignation of the desviation for each variable, in order with the epw variables consulted.\n",
        "        prob_index = 0\n",
        "        for e in range(len(single_shape_list)):\n",
        "            single_shape_list[e] = np.random.normal(single_shape_list[e], desviation[prob_index])\n",
        "            if prob_index == (len(desviation)-1):\n",
        "                prob_index = 0\n",
        "            else:\n",
        "                prob_index += 1\n",
        "\n",
        "        predictions = np.array(single_shape_list)\n",
        "        # The prediction list is transformed in a Numpy Array to concatenate after with the rest of the observation variables.\n",
        "        return predictions\n",
        "\n",
        "\"\"\"Utilities and methods to configurate the execution of the episode in EnergyPlus with RLlib.\n",
        "\"\"\"\n",
        "\n",
        "def episode_epJSON(env_config: dict):\n",
        "    \"\"\"This method define the properties of the episode. Changing some properties as weather or\n",
        "    Run Time Period, and defining others fix properties as volumen or window area relation.\n",
        "\n",
        "    Args:\n",
        "        env_config (dict): Environment configuration.\n",
        "\n",
        "    Return:\n",
        "        dict: The method returns the env_config with modifications.\n",
        "    \"\"\"\n",
        "    if env_config.get('epjson', False) == False:\n",
        "        env_config = epJSON_path(env_config)\n",
        "        # If the path to epjson is not already set, it is set here.\n",
        "    with open(env_config['epjson']) as file:\n",
        "        epJSON_object: dict = json.load(file)\n",
        "        # Establish the epJSON Object, it will be manipulated to modify the building model.\n",
        "\n",
        "    epJSON_object['Building'][next(iter(epJSON_object['Building']))]['north_axis'] = env_config['rotation']\n",
        "    # The building is oriented as is possitioned in the land.\n",
        "\n",
        "    ObjectName = next(iter(epJSON_object['RunPeriod']))\n",
        "    epJSON_object[\"RunPeriod\"][ObjectName][\"begin_month\"] = 1\n",
        "    epJSON_object[\"RunPeriod\"][ObjectName][\"begin_day_of_month\"] = 1\n",
        "    epJSON_object[\"RunPeriod\"][ObjectName][\"end_month\"] = 12\n",
        "    epJSON_object[\"RunPeriod\"][ObjectName][\"end_day_of_month\"] = 31\n",
        "\n",
        "    env_config['construction_u_factor'] = u_factor(epJSON_object)\n",
        "    # The global U factor is calculated.\n",
        "\n",
        "    for key in [key for key in epJSON_object[\"InternalMass\"].keys()]:\n",
        "        epJSON_object[\"InternalMass\"][key][\"surface_area\"] = np.random.randint(10,40) if not env_config['is_test'] else 15\n",
        "        # The internal thermal mass is modified.\n",
        "\n",
        "    env_config['inercial_mass'] = inertial_mass(epJSON_object)\n",
        "    # The total inertial thermal mass is calculated.\n",
        "\n",
        "    env_config['E_max'] = (0.5+(0.5 - 0.08)*np.random.random_sample()) if not env_config['is_test'] else 2.5/6\n",
        "    HVAC_names = [key for key in epJSON_object[\"ZoneHVAC:IdealLoadsAirSystem\"].keys()]\n",
        "    for hvac in range(len(HVAC_names)):\n",
        "        epJSON_object[\"ZoneHVAC:IdealLoadsAirSystem\"][HVAC_names[hvac]][\"maximum_sensible_heating_capacity\"] = env_config['E_max']\n",
        "        epJSON_object[\"ZoneHVAC:IdealLoadsAirSystem\"][HVAC_names[hvac]][\"maximum_total_cooling_capacity\"] = env_config['E_max']\n",
        "        # The limit capacity of bouth cooling and heating are changed.\n",
        "\n",
        "    env_config[\"epjson\"] = f\"{env_config['epjson_output_folder']}/model-{env_config['episode']:08}-{os.getpid():05}.epJSON\"\n",
        "    with open(env_config[\"epjson\"], 'w') as fp:\n",
        "        json.dump(epJSON_object, fp, sort_keys=False, indent=4)\n",
        "        # The new modify epjson file is writed.\n",
        "\n",
        "    env_config['epw'],env_config['latitud'], env_config['longitud'], env_config['altitud'] = weather_file(env_config)\n",
        "    # Assign the epw path and the correspondent latitude, longitude, and altitude.\n",
        "    return env_config\n",
        "\n",
        "def epJSON_path(env_config: dict):\n",
        "    \"\"\"This method define the path to the epJSON file to be simulated.\n",
        "\n",
        "    Args:\n",
        "        env_config (dict): Environment configuration that must contain:\n",
        "            'epjson_folderpath'\n",
        "            'building_name'\n",
        "\n",
        "    Return:\n",
        "        dict: The method returns the env_config with modifications.\n",
        "    \"\"\"\n",
        "    env_config['epjson'] = env_config['epjson_folderpath']+'/'+env_config['building_name']+'.epjson'\n",
        "    return env_config\n",
        "\n",
        "def inertial_mass(epJSON_object: dict[str,dict]):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        epJSON_object (dict[str,dict]): _description_\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "    # se define una lista para almacenar\n",
        "    masas_termicas = []\n",
        "\n",
        "    building_surfaces = [key for key in epJSON_object[\"BuildingSurface:Detailed\"].keys()]\n",
        "    # se obtienen los nombres de las superficies de la envolvente\n",
        "    internal_mass_surfaces = [key for key in epJSON_object[\"InternalMass\"].keys()]\n",
        "\n",
        "    all_building_keys = [key for key in epJSON_object.keys()]\n",
        "    all_material_list = ['Material','Material:NoMass','Material:InfraredTransparent','Material:AirGap',\n",
        "        'Material:RoofVegetation','WindowMaterial:SimpleGlazingSystem','WindowMaterial:Glazing',\n",
        "        'WindowMaterial:GlazingGroup:Thermochromic','WindowMaterial:Glazing:RefractionExtinctionMethod',\n",
        "        'WindowMaterial:Gas','WindowGap:SupportPillar','WindowGap:DeflectionState',\n",
        "        'WindowMaterial:GasMixture','WindowMaterial:Gap'\n",
        "    ]\n",
        "    materials_dict = {}\n",
        "    for material in all_material_list:\n",
        "        if material in all_building_keys:\n",
        "            materials_dict[material] = epJSON_object[material].keys()\n",
        "    # se obtienen los nombres de los diferentes tipos de materiales\n",
        "\n",
        "    # lazo para consultar cada superficie de la envolvente\n",
        "    for surface in building_surfaces:\n",
        "        # se calcula el área de la superficie\n",
        "        area = material_area(epJSON_object,surface)\n",
        "        # se identifica la consutrucción\n",
        "        s_construction = epJSON_object['BuildingSurface:Detailed'][surface]['construction_name']\n",
        "\n",
        "        # se obtiene la densidad del materia: \\rho[kg/m3]\n",
        "        # se obtiene el calor específico del material: C[J/kg°C]\n",
        "        # se calcula el volumen que ocupa el material: V[m3]=area*thickness\n",
        "        # se calcula la masa térmica: M[J/°C] = \\rho[kg/m3] * C[J/kg°C] * V[m3]\n",
        "\n",
        "        # se establece un lazo para calcular la masa térmica de cada capa\n",
        "        m_surface = 0\n",
        "        layers = [key for key in epJSON_object['Construction'][s_construction].keys()]\n",
        "        for layer in layers:\n",
        "            material = epJSON_object['Construction'][s_construction][layer]\n",
        "            material_list = find_dict_key_by_nested_key(\n",
        "                material,\n",
        "                materials_dict\n",
        "            )\n",
        "            # se obtiene el espesor y la conductividad térmica del material de la capa\n",
        "            if material_list == 'Material:NoMass' or material_list == 'Material:AirGap' or material_list == 'Material:InfraredTransparent' or material_list == 'WindowMaterial:Gas':\n",
        "                m_capa = 0\n",
        "            else:\n",
        "                espesor_capa = epJSON_object[material_list][material]['thickness']\n",
        "                calor_especifico_capa = epJSON_object[material_list][material]['specific_heat']\n",
        "                densidad_capa = epJSON_object[material_list][material]['density']\n",
        "                m_capa = area * espesor_capa * calor_especifico_capa * densidad_capa\n",
        "\n",
        "            # se suma la resistencia de la superficie\n",
        "            m_surface += m_capa\n",
        "        # se guarda la resistencia de la superficie\n",
        "        masas_termicas.append(m_surface)\n",
        "\n",
        "    # se suma la masa interna asignada\n",
        "    for surface in internal_mass_surfaces:\n",
        "        # se calcula el área de la superficie\n",
        "        area = epJSON_object['InternalMass'][surface]['surface_area']\n",
        "        # se identifica la consutrucción\n",
        "        s_construction = epJSON_object['InternalMass'][surface]['construction_name']\n",
        "\n",
        "        # se obtiene la densidad del materia: \\rho[kg/m3]\n",
        "        # se obtiene el calor específico del material: C[J/kg°C]\n",
        "        # se calcula el volumen que ocupa el material: V[m3]=area*thickness\n",
        "        # se calcula la masa térmica: M[J/°C] = \\rho[kg/m3] * C[J/kg°C] * V[m3]\n",
        "\n",
        "        # se establece un lazo para calcular la masa térmica de cada capa\n",
        "        m_surface = 0\n",
        "        layers = [key for key in epJSON_object['Construction'][s_construction].keys()]\n",
        "        for layer in layers:\n",
        "            material = epJSON_object['Construction'][s_construction][layer]\n",
        "            material_list = find_dict_key_by_nested_key(\n",
        "                material,\n",
        "                materials_dict\n",
        "            )\n",
        "            # se obtiene el espesor y la conductividad térmica del material de la capa\n",
        "            if material_list == 'Material:NoMass' or material_list == 'Material:AirGap' or material_list == 'Material:InfraredTransparent' or material_list == 'WindowMaterial:Gas':\n",
        "                m_capa = 0\n",
        "            else:\n",
        "                espesor_capa = epJSON_object[material_list][material]['thickness']\n",
        "                calor_especifico_capa = epJSON_object[material_list][material]['specific_heat']\n",
        "                densidad_capa = epJSON_object[material_list][material]['density']\n",
        "                m_capa = area * espesor_capa * calor_especifico_capa * densidad_capa\n",
        "\n",
        "            # se suma la resistencia de la superficie\n",
        "            m_surface += m_capa\n",
        "        # se guarda la resistencia de la superficie\n",
        "        masas_termicas.append(m_surface)\n",
        "\n",
        "    # Cálculo de la masa termica total\n",
        "    M_total = 0\n",
        "    for m in range(0,len(masas_termicas)-1,1):\n",
        "        M_total += masas_termicas[m]\n",
        "\n",
        "    return M_total\n",
        "\n",
        "def u_factor(epJSON_object: dict[str,dict]):\n",
        "    \"\"\"This function select all the building surfaces and fenestration surfaces and calculate the\n",
        "    global U-factor of the building, like EnergyPlus does.\n",
        "    \"\"\"\n",
        "    # se define una lista para almacenar las resistencias de cada supercie\n",
        "    resistences = []\n",
        "    areas = []\n",
        "    # se obtienen los nombres de las superficies de la envolvente\n",
        "    building_surfaces = [key for key in epJSON_object['BuildingSurface:Detailed'].keys()]\n",
        "    fenestration_surfaces = [key for key in epJSON_object['FenestrationSurface:Detailed'].keys()]\n",
        "    # se obtienen los nombres de los diferentes tipos de materiales\n",
        "\n",
        "    all_building_keys = [key for key in epJSON_object.keys()]\n",
        "    all_material_list = ['Material','Material:NoMass','Material:InfraredTransparent','Material:AirGap',\n",
        "        'Material:RoofVegetation','WindowMaterial:SimpleGlazingSystem','WindowMaterial:Glazing',\n",
        "        'WindowMaterial:GlazingGroup:Thermochromic','WindowMaterial:Glazing:RefractionExtinctionMethod',\n",
        "        'WindowMaterial:Gas','WindowGap:SupportPillar','WindowGap:DeflectionState',\n",
        "        'WindowMaterial:GasMixture','WindowMaterial:Gap'\n",
        "    ]\n",
        "    materials_dict = {}\n",
        "    for material in all_material_list:\n",
        "        if material in all_building_keys:\n",
        "            materials_dict[material] = epJSON_object[material].keys()\n",
        "    # se obtienen los nombres de los diferentes tipos de materiales\n",
        "\n",
        "    # lazo para consultar cada superficie de la envolvente\n",
        "    for surface in building_surfaces:\n",
        "        # se calcula el área de la superficie\n",
        "        areas.append(material_area(epJSON_object,surface))\n",
        "        # se identifica la consutrucción\n",
        "        s_construction = epJSON_object['BuildingSurface:Detailed'][surface]['construction_name']\n",
        "        # se establece un lazo para calcular la resistencia de cada capa\n",
        "        r_surface = 0\n",
        "        layers = [key for key in epJSON_object['Construction'][s_construction].keys()]\n",
        "        for layer in layers:\n",
        "            material = epJSON_object['Construction'][s_construction][layer]\n",
        "            material_list = find_dict_key_by_nested_key(\n",
        "                material,\n",
        "                materials_dict\n",
        "            )\n",
        "            # se obtiene el espesor y la conductividad térmica del material de la capa\n",
        "            if material_list == 'Material:NoMass' or material_list == 'Material:AirGap':\n",
        "                r_capa = epJSON_object[material_list][material]['thermal_resistance']\n",
        "            elif material_list == 'Material:InfraredTransparent':\n",
        "                r_capa = 0\n",
        "            elif material_list == 'WindowMaterial:Gas':\n",
        "                espesor_capa = epJSON_object[material_list][material]['thickness']\n",
        "                if epJSON_object[material_list][material]['gas_type'] == 'Air':\n",
        "                    conductividad_capa = 0.0257\n",
        "                elif epJSON_object[material_list][material]['gas_type'] == 'Argon':\n",
        "                    conductividad_capa = 0.0162\n",
        "                elif epJSON_object[material_list][material]['gas_type'] == 'Xenon':\n",
        "                    conductividad_capa = 0.00576\n",
        "                elif epJSON_object[material_list][material]['gas_type'] == 'Krypton':\n",
        "                    conductividad_capa = 0.00943\n",
        "                else:\n",
        "                    print('El nombre del gas no corresponde con los que pueden utilizarse: Air, Argon, Xenon, Krypton.')\n",
        "                    NameError\n",
        "                r_capa = espesor_capa/conductividad_capa\n",
        "            else:\n",
        "                espesor_capa = epJSON_object[material_list][material]['thickness']\n",
        "                conductividad_capa = epJSON_object[material_list][material]['conductivity']\n",
        "                r_capa = espesor_capa/conductividad_capa\n",
        "\n",
        "            # se suma la resistencia de la superficie\n",
        "            r_surface += r_capa\n",
        "        # se guarda la resistencia de la superficie\n",
        "        resistences.append(r_surface)\n",
        "\n",
        "    # lazo para consultar cada superfice de fenestración\n",
        "    for fenestration in fenestration_surfaces:\n",
        "        # se calcula el área de la superficie\n",
        "        areas.append(fenestration_area(epJSON_object, fenestration))\n",
        "        # se identifica la consutrucción\n",
        "        s_construction = epJSON_object['FenestrationSurface:Detailed'][fenestration]['construction_name']\n",
        "        # se establece un lazo para calcular la resistencia de cada capa\n",
        "        r_surface = 0\n",
        "        layers = [key for key in epJSON_object['Construction'][s_construction].keys()]\n",
        "        for layer in layers:\n",
        "            material = epJSON_object['Construction'][s_construction][layer]\n",
        "            material_list = find_dict_key_by_nested_key(\n",
        "                material,\n",
        "                materials_dict\n",
        "            )\n",
        "            # se obtiene el espesor y la conductividad térmica del material de la capa\n",
        "            if material_list == 'Material:NoMass' or material_list == 'Material:AirGap':\n",
        "                r_capa = epJSON_object[material_list][material]['thermal_resistance']\n",
        "            elif material_list == 'Material:InfraredTransparent':\n",
        "                r_capa = 0\n",
        "            elif material_list == 'WindowMaterial:Gas':\n",
        "                espesor_capa = epJSON_object[material_list][material]['thickness']\n",
        "                if epJSON_object[material_list][material]['gas_type'] == 'Air':\n",
        "                    conductividad_capa = 0.0257\n",
        "                elif epJSON_object[material_list][material]['gas_type'] == 'Argon':\n",
        "                    conductividad_capa = 0.0162\n",
        "                elif epJSON_object[material_list][material]['gas_type'] == 'Xenon':\n",
        "                    conductividad_capa = 0.00576\n",
        "                elif epJSON_object[material_list][material]['gas_type'] == 'Krypton':\n",
        "                    conductividad_capa = 0.00943\n",
        "                else:\n",
        "                    print('El nombre del gas no corresponde con los que pueden utilizarse: Air, Argon, Xenon, Krypton.')\n",
        "                    NameError\n",
        "                r_capa = espesor_capa/conductividad_capa\n",
        "            else:\n",
        "                espesor_capa = epJSON_object[material_list][material]['thickness']\n",
        "                conductividad_capa = epJSON_object[material_list][material]['conductivity']\n",
        "                r_capa = espesor_capa/conductividad_capa\n",
        "\n",
        "            # se suma la resistencia de la superficie\n",
        "            r_surface += r_capa\n",
        "        # se guarda la resistencia de la superficie\n",
        "        resistences.append(r_surface)\n",
        "\n",
        "    # Cálculo de U-Factor en W/°C\n",
        "    u_factor = 0\n",
        "    for n in range(0, len(areas)-1,1):\n",
        "        u_factor =+ areas[n]/resistences[n]\n",
        "\n",
        "    return u_factor\n",
        "\n",
        "def material_area(epJSON_object, nombre_superficie):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        epJSON_object (_type_): _description_\n",
        "        nombre_superficie (_type_): _description_\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "    # Calcula dos vectores que forman dos lados del cuadrilátero\n",
        "    vector1 = [\n",
        "        epJSON_object['BuildingSurface:Detailed'][nombre_superficie]['vertices'][1]['vertex_x_coordinate'] - epJSON_object['BuildingSurface:Detailed'][nombre_superficie]['vertices'][0]['vertex_x_coordinate'],\n",
        "        epJSON_object['BuildingSurface:Detailed'][nombre_superficie]['vertices'][1]['vertex_y_coordinate'] - epJSON_object['BuildingSurface:Detailed'][nombre_superficie]['vertices'][0]['vertex_y_coordinate'],\n",
        "        epJSON_object['BuildingSurface:Detailed'][nombre_superficie]['vertices'][1]['vertex_z_coordinate'] - epJSON_object['BuildingSurface:Detailed'][nombre_superficie]['vertices'][0]['vertex_z_coordinate']\n",
        "    ]\n",
        "    vector2 = [\n",
        "        epJSON_object['BuildingSurface:Detailed'][nombre_superficie]['vertices'][2]['vertex_x_coordinate'] - epJSON_object['BuildingSurface:Detailed'][nombre_superficie]['vertices'][0]['vertex_x_coordinate'],\n",
        "        epJSON_object['BuildingSurface:Detailed'][nombre_superficie]['vertices'][2]['vertex_y_coordinate'] - epJSON_object['BuildingSurface:Detailed'][nombre_superficie]['vertices'][0]['vertex_y_coordinate'],\n",
        "        epJSON_object['BuildingSurface:Detailed'][nombre_superficie]['vertices'][2]['vertex_z_coordinate'] - epJSON_object['BuildingSurface:Detailed'][nombre_superficie]['vertices'][0]['vertex_z_coordinate']\n",
        "    ]\n",
        "\n",
        "    # Calcula el producto vectorial entre los dos vectores\n",
        "    producto_vectorial = [\n",
        "        vector1[1] * vector2[2] - vector1[2] * vector2[1],\n",
        "        vector1[2] * vector2[0] - vector1[0] * vector2[2],\n",
        "        vector1[0] * vector2[1] - vector1[1] * vector2[0]\n",
        "    ]\n",
        "\n",
        "    # Calcula el módulo del producto vectorial como el área del cuadrilátero\n",
        "    area = 0.5 * (abs(producto_vectorial[0]) + abs(producto_vectorial[1]) + abs(producto_vectorial[2]))\n",
        "    return area\n",
        "\n",
        "def fenestration_area(epJSON_object, fenestration):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        epJSON_object (_type_): _description_\n",
        "        fenestration (_type_): _description_\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "\n",
        "    # Calcula dos vectores que forman dos lados del cuadrilátero\n",
        "    vector1 = [\n",
        "        epJSON_object['FenestrationSurface:Detailed'][fenestration]['vertex_2_x_coordinate'] - epJSON_object['FenestrationSurface:Detailed'][fenestration]['vertex_1_x_coordinate'],\n",
        "        epJSON_object['FenestrationSurface:Detailed'][fenestration]['vertex_2_y_coordinate'] - epJSON_object['FenestrationSurface:Detailed'][fenestration]['vertex_1_y_coordinate'],\n",
        "        epJSON_object['FenestrationSurface:Detailed'][fenestration]['vertex_2_z_coordinate'] - epJSON_object['FenestrationSurface:Detailed'][fenestration]['vertex_1_z_coordinate']\n",
        "    ]\n",
        "    vector2 = [\n",
        "        epJSON_object['FenestrationSurface:Detailed'][fenestration]['vertex_3_x_coordinate'] - epJSON_object['FenestrationSurface:Detailed'][fenestration]['vertex_1_x_coordinate'],\n",
        "        epJSON_object['FenestrationSurface:Detailed'][fenestration]['vertex_3_y_coordinate'] - epJSON_object['FenestrationSurface:Detailed'][fenestration]['vertex_1_y_coordinate'],\n",
        "        epJSON_object['FenestrationSurface:Detailed'][fenestration]['vertex_3_z_coordinate'] - epJSON_object['FenestrationSurface:Detailed'][fenestration]['vertex_1_z_coordinate']\n",
        "    ]\n",
        "\n",
        "    # Calcula el producto vectorial entre los dos vectores\n",
        "    producto_vectorial = [\n",
        "        vector1[1] * vector2[2] - vector1[2] * vector2[1],\n",
        "        vector1[2] * vector2[0] - vector1[0] * vector2[2],\n",
        "        vector1[0] * vector2[1] - vector1[1] * vector2[0]\n",
        "    ]\n",
        "\n",
        "    # Calcula el módulo del producto vectorial como el área del cuadrilátero\n",
        "    area = 0.5 * (abs(producto_vectorial[0]) + abs(producto_vectorial[1]) + abs(producto_vectorial[2]))\n",
        "    return area\n",
        "\n",
        "def find_dict_key_by_nested_key(key, lists_dict):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        key (_type_): _description_\n",
        "        lists_dict (_type_): _description_\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "    for dict_key, lst in lists_dict.items():\n",
        "        if key in lst:\n",
        "            return dict_key\n",
        "    return None\n",
        "\n",
        "\"\"\"# ENERGYPLUS RUNNER\n",
        "\n",
        "This script contain the EnergyPlus Runner that execute EnergyPlus from its Python API in the version\n",
        "23.2.0.\n",
        "\"\"\"\n",
        "\n",
        "class EnergyPlusRunner:\n",
        "    \"\"\"This object have the particularity of `start` EnergyPlus, `_collect_obs` and `_send_actions` to\n",
        "    send it trhougt queue to the EnergyPlus Environment thread.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        episode: int,\n",
        "        env_config: Dict[str, Any],\n",
        "        obs_queue: Queue,\n",
        "        act_queue: Queue,\n",
        "        cooling_queue: Queue,\n",
        "        heating_queue: Queue,\n",
        "        pmv_queue: Queue,\n",
        "        ppd_queue: Queue,\n",
        "        beta_queue: Queue,\n",
        "        emax_queue: Queue,\n",
        "        ):\n",
        "        \"\"\"The object has an intensive interaction with EnergyPlus Environment script, exchange information\n",
        "        between two threads. For a good coordination queue events are stablished and different canals of\n",
        "        information are defined.\n",
        "\n",
        "        Args:\n",
        "            episode (int): Episode number.\n",
        "            env_config (Dict[str, Any]): Environment configuration defined in the call to the EnergyPlus Environment.\n",
        "            obs_queue (Queue): Queue object definition.\n",
        "            act_queue (Queue): Queue object definition.\n",
        "            cooling_queue (Queue): Queue object definition.\n",
        "            heating_queue (Queue): Queue object definition.\n",
        "            pmv_queue (Queue): Queue object definition.\n",
        "            ppd_queue (Queue): Queue object definition.\n",
        "            beta_queue (Queue): Queue object definition.\n",
        "            emax_queue (Queue): Queue object definition.\n",
        "\n",
        "        Return:\n",
        "            None.\n",
        "        \"\"\"\n",
        "        self.episode = episode\n",
        "        self.env_config = env_config\n",
        "        self.env_config['episode'] = self.episode\n",
        "        self.obs_queue = obs_queue\n",
        "        self.act_queue = act_queue\n",
        "        self.cooling_queue = cooling_queue\n",
        "        self.heating_queue = heating_queue\n",
        "        self.pmv_queue = pmv_queue\n",
        "        self.ppd_queue = ppd_queue\n",
        "        self.beta_queue = beta_queue\n",
        "        self.emax_queue = emax_queue\n",
        "        # Asignation of variables.\n",
        "\n",
        "        self.obs_event = threading.Event()\n",
        "        self.act_event = threading.Event()\n",
        "        self.cooling_event = threading.Event()\n",
        "        self.heating_event = threading.Event()\n",
        "        self.pmv_event = threading.Event()\n",
        "        self.ppd_event = threading.Event()\n",
        "        self.beta_event = threading.Event()\n",
        "        self.emax_event = threading.Event()\n",
        "        # The queue events are generated.\n",
        "\n",
        "        self.energyplus_exec_thread: Optional[threading.Thread] = None\n",
        "        self.energyplus_state: Any = None\n",
        "        self.sim_results: int = 0\n",
        "        self.initialized = False\n",
        "        self.init_handles = False\n",
        "        self.simulation_complete = False\n",
        "        self.first_observation = True\n",
        "        # Variables to be used in this thread.\n",
        "\n",
        "        self.env_config = epJSON_path(self.env_config)\n",
        "        # The path for the epjson file is defined.\n",
        "\n",
        "        self.variables = {\n",
        "            \"To\": (\"Site Outdoor Air Drybulb Temperature\", \"Environment\"), #0\n",
        "            \"Ti\": (\"Zone Mean Air Temperature\", \"Thermal Zone\"), #1\n",
        "            \"v\": (\"Site Wind Speed\", \"Environment\"), #2\n",
        "            \"d\": (\"Site Wind Direction\", \"Environment\"), #3\n",
        "            \"RHo\": (\"Site Outdoor Air Relative Humidity\", \"Environment\"), #4\n",
        "            \"RHi\": (\"Zone Air Relative Humidity\", \"Thermal Zone\"), #5\n",
        "            \"T_rad\": (\"Zone Mean Radiant Temperature\", \"Thermal Zone\"), #del\n",
        "            \"Fanger_PMV\":(\"Zone Thermal Comfort Fanger Model PMV\", \"People\"), #del\n",
        "            \"Fanger_PPD\":(\"Zone Thermal Comfort Fanger Model PPD\", \"People\"), #del\n",
        "        }\n",
        "        self.var_handles: Dict[str, int] = {}\n",
        "        # Declaration of variables this simulation will interact with.\n",
        "\n",
        "        self.meters = {\n",
        "            \"dh\": \"Heating:DistrictHeatingWater\", #6\n",
        "            \"dc\": \"Cooling:DistrictCooling\" #7\n",
        "        }\n",
        "        self.meter_handles: Dict[str, int] = {}\n",
        "        # Declaration of meters this simulation will interact with.\n",
        "\n",
        "        self.actuators = {\n",
        "            \"opening_window_1\": (\"AirFlow Network Window/Door Opening\", \"Venting Opening Factor\", \"window_2\"), # 8: opening factor between 0.0 and 1.0\n",
        "            \"opening_window_2\": (\"AirFlow Network Window/Door Opening\", \"Venting Opening Factor\", \"window_3\"), # 9: opening factor between 0.0 and 1.0\n",
        "        }\n",
        "        self.actuator_handles: Dict[str, int] = {}\n",
        "        # Declaration of actuators this simulation will interact with.\n",
        "        # Airflow Network Openings (EnergyPlus Documentation)\n",
        "        # An actuator called “AirFlow Network Window/Door Opening” is available with a control type\n",
        "        # called “Venting Opening Factor.” It is available in models that have operable openings in the Airflow\n",
        "        # Network model and that are entered by using either AirflowNetwork:MultiZone:Component:DetailedOpening,\n",
        "        # AirflowNetwork:MultiZone:Component:SimpleOpening, or AirflowNetwork:MultiZone:Component:HorizontalOpening\n",
        "        # input objects. This control allows you to use EMS to vary the size of the opening during the\n",
        "        # airflow model calculations, such as for natural and hybrid ventilation.\n",
        "        # The unique identifier is the name of the surface (window, door or air boundary), not the name of\n",
        "        # the associated airflow network input objects. The actuator control involves setting the value of the\n",
        "        # opening factor between 0.0 and 1.0. Use of this actuator with an air boundary surface is allowed,\n",
        "        # but will generate a warning since air boundaries are typically always open.\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"This method inicialize EnergyPlus. First the episode is configurate, the calling functions\n",
        "        established and the thread is generated here.\n",
        "        \"\"\"\n",
        "        self.env_config = episode_epJSON(self.env_config)\n",
        "        # Configurate the episode.\n",
        "\n",
        "        self.weather_stats = Probabilities(self.env_config)\n",
        "        # Specify the weather statisitical file.\n",
        "\n",
        "        self.energyplus_state = api.state_manager.new_state()\n",
        "        # Start a new EnergyPlus state (condition for execute EnergyPlus Python API).\n",
        "\n",
        "        api.runtime.callback_begin_system_timestep_before_predictor(self.energyplus_state, self._collect_first_obs)\n",
        "        # Collect the first observation. This is execute only once at the begginig of the episode.\n",
        "        # The calling point called “BeginTimestepBeforePredictor” occurs near the beginning of each timestep\n",
        "        # but before the predictor executes. “Predictor” refers to the step in EnergyPlus modeling when the\n",
        "        # zone loads are calculated. This calling point is useful for controlling components that affect the\n",
        "        # thermal loads the HVAC systems will then attempt to meet. Programs called from this point\n",
        "        # might actuate internal gains based on current weather or on the results from the previous timestep.\n",
        "        # Demand management routines might use this calling point to reduce lighting or process loads,\n",
        "        # change thermostat settings, etc.\n",
        "\n",
        "        api.runtime.callback_begin_zone_timestep_after_init_heat_balance(self.energyplus_state, self._send_actions)\n",
        "        # Execute the actions in the environment.\n",
        "        # The calling point called “BeginZoneTimestepAfterInitHeatBalance” occurs at the beginning of each\n",
        "        # timestep after “InitHeatBalance” executes and before “ManageSurfaceHeatBalance”. “InitHeatBalance” refers to the step in EnergyPlus modeling when the solar shading and daylighting coefficients\n",
        "        # are calculated. This calling point is useful for controlling components that affect the building envelope including surface constructions and window shades. Programs called from this point might\n",
        "        # actuate the building envelope or internal gains based on current weather or on the results from the\n",
        "        # previous timestep. Demand management routines might use this calling point to operate window\n",
        "        # shades, change active window constructions, etc. This calling point would be an appropriate place\n",
        "        # to modify weather data values.\n",
        "\n",
        "        api.runtime.callback_end_zone_timestep_after_zone_reporting(self.energyplus_state, self._collect_obs)\n",
        "        # Collect the observations after the action executions and use them to provide new actions.\n",
        "        # The calling point called “EndOfZoneTimestepAfterZoneReporting” occurs at the end of a zone\n",
        "        # timestep after output variable reporting is finalized. It is useful for preparing calculations that\n",
        "        # will go into effect the next timestep. Its capabilities are similar to BeginTimestepBeforePredictor,\n",
        "        # except that input data for current time, date, and weather data align with different timesteps.\n",
        "\n",
        "        api.runtime.set_console_output_status(self.energyplus_state, self.env_config['ep_terminal_output'])\n",
        "        # Control of the console printing process.\n",
        "\n",
        "        def _run_energyplus():\n",
        "            \"\"\"Run EnergyPlus in a non-blocking way with Threads.\n",
        "            \"\"\"\n",
        "            cmd_args = self.make_eplus_args()\n",
        "            print(f\"running EnergyPlus with args: {cmd_args}\")\n",
        "            self.sim_results = api.runtime.run_energyplus(self.energyplus_state, cmd_args)\n",
        "            self.simulation_complete = True\n",
        "\n",
        "        self.energyplus_exec_thread = threading.Thread(\n",
        "            target=_run_energyplus,\n",
        "            args=()\n",
        "        )\n",
        "        self.energyplus_exec_thread.start()\n",
        "        # Here the thread is divide in two.\n",
        "\n",
        "    def _collect_obs(self, state_argument):\n",
        "        \"\"\"EnergyPlus callback that collects output variables, meters and actuator actions\n",
        "        values and enqueue them to the EnergyPlus Environment thread.\n",
        "        \"\"\"\n",
        "        if self.simulation_complete or not self._init_callback(state_argument):\n",
        "            # To not perform observations when the episode is ended or if the callbacks and the\n",
        "            # warming period are not complete.\n",
        "            return\n",
        "\n",
        "        time_step = api.exchange.zone_time_step_number(state_argument)\n",
        "        hour = api.exchange.hour(state_argument)\n",
        "        simulation_day = api.exchange.day_of_year(state_argument)\n",
        "        # Timestep variables.\n",
        "        obs = {\n",
        "            **{\n",
        "                key: api.exchange.get_variable_value(state_argument, handle)\n",
        "                for key, handle\n",
        "                in self.var_handles.items()\n",
        "            },\n",
        "            **{\n",
        "                key: api.exchange.get_meter_value(state_argument, handle)\n",
        "                for key, handle\n",
        "                in self.meter_handles.items()\n",
        "            },\n",
        "            **{\n",
        "                key: api.exchange.get_actuator_value(state_argument, handle)\n",
        "                for key, handle\n",
        "                in self.actuator_handles.items()\n",
        "            }\n",
        "        }\n",
        "        # Variables, meters and actuatos conditions as observation.\n",
        "        obs.update(\n",
        "            {\n",
        "            'hora': hour,#10\n",
        "            'simulation_day': simulation_day,#11\n",
        "            'volumen': self.env_config['volumen'],#12\n",
        "            'window_area_relation_north': self.env_config['window_area_relation_north'],#13\n",
        "            'window_area_relation_west': self.env_config['window_area_relation_west'],#14\n",
        "            'window_area_relation_south': self.env_config['window_area_relation_south'],#15\n",
        "            'window_area_relation_east': self.env_config['window_area_relation_east'],#16\n",
        "            'construction_u_factor': self.env_config['construction_u_factor'], #17\n",
        "            'inercial_mass': self.env_config['inercial_mass'], #18\n",
        "            'latitud': self.env_config['latitud'], #19\n",
        "            'longitud':self.env_config['longitud'], #20\n",
        "            'altitud': self.env_config['altitud'], #21\n",
        "            'beta': self.env_config['beta'], #22\n",
        "            'E_max': self.env_config['E_max'], #23\n",
        "            \"rad\": api.exchange.today_weather_beam_solar_at_time(state_argument, hour, time_step), #24\n",
        "            }\n",
        "        )\n",
        "        # Upgrade of the timestep observation.\n",
        "\n",
        "        self.cooling_queue.put(obs['dc'])\n",
        "        self.cooling_event.set()\n",
        "        self.heating_queue.put(obs['dh'])\n",
        "        self.heating_event.set()\n",
        "        self.beta_queue.put(obs['beta'])\n",
        "        self.beta_event.set()\n",
        "        self.emax_queue.put(obs['E_max'])\n",
        "        self.emax_event.set()\n",
        "        self.pmv_queue.put(obs[\"Fanger_PMV\"])\n",
        "        self.pmv_event.set()\n",
        "        self.ppd_queue.put(obs[\"Fanger_PPD\"])\n",
        "        self.ppd_event.set()\n",
        "        # Set the variables to communicate with queue before to delete the following.\n",
        "\n",
        "        del obs[\"T_rad\"]\n",
        "        del obs[\"Fanger_PMV\"]\n",
        "        del obs[\"Fanger_PPD\"]\n",
        "        # Variables are deleted from the observation because are difficult to mesure.\n",
        "\n",
        "        next_obs = np.array(list(obs.values()))\n",
        "        # Transform the observation in a numpy array to meet the condition expected in a RLlib Environment\n",
        "        weather_prob = self.weather_stats.ten_days_predictions(simulation_day)\n",
        "        # Consult the stadistics of the weather to put into the obs array. This add 1440 elements to the observation.\n",
        "        self.next_obs = np.concatenate([next_obs, weather_prob])\n",
        "\n",
        "        self.obs_queue.put(self.next_obs)\n",
        "        self.obs_event.set()\n",
        "        # Set the observation to communicate with queue.\n",
        "\n",
        "    def _collect_first_obs(self, state_argument):\n",
        "        \"\"\"This method is used to collect only the first observation of the environment when the episode beggins.\n",
        "\n",
        "        Args:\n",
        "            state_argument (c_void_p): EnergyPlus state pointer. This is created with `api.state_manager.new_state()`.\n",
        "        \"\"\"\n",
        "        if self.first_observation:\n",
        "            self._collect_obs(state_argument)\n",
        "            self.first_observation = False\n",
        "        else:\n",
        "            return\n",
        "\n",
        "    def _send_actions(self, state_argument):\n",
        "        \"\"\"EnergyPlus callback that sets actuator value from last decided action\n",
        "        \"\"\"\n",
        "        if self.simulation_complete or not self._init_callback(state_argument):\n",
        "            # To not perform actions when the episode is ended or if the callbacks and the\n",
        "            # warming period are not complete.\n",
        "            return\n",
        "\n",
        "        self.act_event.wait(20)\n",
        "        # Wait for an action.\n",
        "        if self.act_queue.empty():\n",
        "            # Return in the first timestep.\n",
        "            return\n",
        "\n",
        "        next_central_action = self.act_queue.get()\n",
        "        # Get the central action from the EnergyPlus Environment `step` method.\n",
        "        # In the case of simple agent a int value and for multiagents a dictionary.\n",
        "        # TODO: Make this EPRunner abble to simple and multi-agent configuration and for natural\n",
        "        # ventilation, shadow control or a integrate control.\n",
        "        next_action = natural_ventilation_action(next_central_action)\n",
        "        # Transform the centraliced action into a list of descentraliced actions.\n",
        "\n",
        "        api.exchange.set_actuator_value(\n",
        "            state=state_argument,\n",
        "            actuator_handle=self.actuator_handles[\"opening_window_1\"],\n",
        "            actuator_value=next_action[0]\n",
        "        )\n",
        "        api.exchange.set_actuator_value(\n",
        "            state=state_argument,\n",
        "            actuator_handle=self.actuator_handles[\"opening_window_2\"],\n",
        "            actuator_value=next_action[1]\n",
        "        )\n",
        "        # Perform the actions in EnergyPlus simulation.\n",
        "\n",
        "    def _init_callback(self, state_argument):\n",
        "        \"\"\"Initialize EnergyPlus handles and checks if simulation runtime is ready\"\"\"\n",
        "        self.init_handles = self._init_handles(state_argument)\n",
        "        self.initialized = self.init_handles \\\n",
        "            and not api.exchange.warmup_flag(state_argument)\n",
        "        return self.initialized\n",
        "\n",
        "    def _init_handles(self, state_argument):\n",
        "        \"\"\"Initialize sensors/actuators handles to interact with during simulation\"\"\"\n",
        "        if not self.init_handles:\n",
        "            if not api.exchange.api_data_fully_ready(state_argument):\n",
        "                return False\n",
        "\n",
        "            self.var_handles = {\n",
        "                key: api.exchange.get_variable_handle(state_argument, *var)\n",
        "                for key, var in self.variables.items()\n",
        "            }\n",
        "            self.meter_handles = {\n",
        "                key: api.exchange.get_meter_handle(state_argument, meter)\n",
        "                for key, meter in self.meters.items()\n",
        "            }\n",
        "            self.actuator_handles = {\n",
        "                key: api.exchange.get_actuator_handle(state_argument, *actuator)\n",
        "                for key, actuator in self.actuators.items()\n",
        "            }\n",
        "            for handles in [\n",
        "                self.var_handles,\n",
        "                self.meter_handles,\n",
        "                self.actuator_handles\n",
        "            ]:\n",
        "                if any([v == -1 for v in handles.values()]):\n",
        "                    available_data = api.exchange.list_available_api_data_csv(state_argument).decode('utf-8')\n",
        "                    print(\n",
        "                        f\"got -1 handle, check your var/meter/actuator names:\\n\"\n",
        "                        f\"> variables: {self.var_handles}\\n\"\n",
        "                        f\"> meters: {self.meter_handles}\\n\"\n",
        "                        f\"> actuators: {self.actuator_handles}\\n\"\n",
        "                        f\"> available E+ API data: {available_data}\"\n",
        "                    )\n",
        "\n",
        "            self.init_handles = True\n",
        "        return True\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Method to stop EnergyPlus simulation and joint the threads.\n",
        "        \"\"\"\n",
        "        if not self.simulation_complete:\n",
        "            self.simulation_complete = True\n",
        "        sleep(3)\n",
        "        self._flush_queues()\n",
        "        self.energyplus_exec_thread.join()\n",
        "        self.energyplus_exec_thread = None\n",
        "        self.first_observation = True\n",
        "        api.runtime.clear_callbacks()\n",
        "        api.state_manager.delete_state(self.energyplus_state)\n",
        "\n",
        "    def failed(self):\n",
        "        \"\"\"This method tells if a EnergyPlus simulations was finished successfully or not.\n",
        "\n",
        "        Returns:\n",
        "            bool: Boolean value of the success of the simulation.\n",
        "        \"\"\"\n",
        "        return self.sim_results != 0\n",
        "\n",
        "    def make_eplus_args(self):\n",
        "        \"\"\"Make command line arguments to pass to EnergyPlus\n",
        "        \"\"\"\n",
        "        eplus_args = [\"-r\"] if self.env_config.get(\"csv\", False) else []\n",
        "        eplus_args += [\n",
        "            \"-w\",\n",
        "            self.env_config[\"epw\"],\n",
        "            \"-d\",\n",
        "            f\"{self.env_config['output']}/episode-{self.episode:08}-{os.getpid():05}\",\n",
        "            self.env_config[\"epjson\"]\n",
        "        ]\n",
        "        return eplus_args\n",
        "\n",
        "    def _flush_queues(self):\n",
        "        \"\"\"Method to liberate the space in the different queue objects.\n",
        "        \"\"\"\n",
        "        for q in [self.obs_queue, self.act_queue, self.cooling_queue,\n",
        "                  self.heating_queue, self.pmv_queue, self.ppd_queue,\n",
        "                  self.beta_queue, self.emax_queue]:\n",
        "            while not q.empty():\n",
        "                q.get()\n",
        "\n",
        "\n",
        "\"\"\"# ENERGYPLUS RLLIB ENVIRONMENT\n",
        "\n",
        "This script define the environment of EnergyPlus implemented in RLlib. To works need to define the\n",
        "EnergyPlus Runner.\n",
        "\"\"\"\n",
        "\n",
        "class EnergyPlusEnv_v0(gym.Env):\n",
        "    def __init__(\n",
        "        self,\n",
        "        env_config\n",
        "        ):\n",
        "        \"\"\"Environment of a building that run with EnergyPlus Runner.\n",
        "\n",
        "        Args:\n",
        "            env_config (Dict[str, Any]): _description_\n",
        "                'action_space'\n",
        "                'observation_space'\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # super init of the base class gym.Env.\n",
        "        self.env_config = env_config\n",
        "        # asigning the configuration of the environment.\n",
        "        self.episode = -1\n",
        "        # variable for the registry of the episode number.\n",
        "        self.action_space = self.env_config['action_space']\n",
        "        # asignation of the action space.\n",
        "        self.observation_space = self.env_config['observation_space']\n",
        "        # asignation of the observation space.\n",
        "        self.last_obs = {}\n",
        "        # dict to save the last observation in the environment.\n",
        "        self.last_beta = 0.\n",
        "        # variable to save the last beta in the environment.\n",
        "        self.last_emax = 0.\n",
        "        # variable to save the last emax in the environment.\n",
        "        self.energyplus_runner: Optional[EnergyPlusRunner] = None\n",
        "        # variable where the EnergyPlus Runner object will be save.\n",
        "        self.obs_queue: Optional[Queue] = None\n",
        "        # queue for observation communication between threads.\n",
        "        self.act_queue: Optional[Queue] = None\n",
        "        # queue for actions communication between threads.\n",
        "        self.cooling_queue: Optional[Queue] = None\n",
        "        # queue for cooling metric communication between threads.\n",
        "        self.heating_queue: Optional[Queue] = None\n",
        "        # queue for heating metric communication between threads.\n",
        "        self.pmv_queue: Optional[Queue] = None\n",
        "        # queue for PMV metric communication between threads.\n",
        "        self.ppd_queue: Optional[Queue] = None\n",
        "        # queue for PPD metric communication between threads.\n",
        "        self.beta_queue: Optional[Queue] = None\n",
        "        # queue for beta value communication between threads. Used in reward function.\n",
        "        self.emax_queue: Optional[Queue] = None\n",
        "        # queue for E_max value communication between threads. Used in reward function.\n",
        "\n",
        "        self.truncate_flag = False\n",
        "\n",
        "    def reset(\n",
        "        self, *,\n",
        "        seed: Optional[int] = None,\n",
        "        options: Optional[Dict[str, Any]] = None\n",
        "    ):\n",
        "        self.episode += 1\n",
        "        # Increment the counting of episodes in 1.\n",
        "        self.timestep = 0\n",
        "\n",
        "        if not self.truncate_flag:\n",
        "\n",
        "            if self.energyplus_runner is not None and self.energyplus_runner.simulation_complete:\n",
        "                # Condition implemented to restart a new epsiode when simulation is completed and EnergyPlus Runner is already inicialized.\n",
        "                self.energyplus_runner.stop()\n",
        "\n",
        "            # If the EnergyPlus Runner is not inicialized is a new simulation run.\n",
        "            self.obs_queue = Queue(maxsize=1)\n",
        "            self.act_queue = Queue(maxsize=1)\n",
        "            self.cooling_queue = Queue(maxsize=1)\n",
        "            self.heating_queue = Queue(maxsize=1)\n",
        "            self.pmv_queue = Queue(maxsize=1)\n",
        "            self.ppd_queue = Queue(maxsize=1)\n",
        "            self.beta_queue = Queue(maxsize=1)\n",
        "            self.emax_queue = Queue(maxsize=1)\n",
        "            # Define the queues for flow control between threads in a max size of 1 because EnergyPlus\n",
        "            # time step will be processed at a time.\n",
        "\n",
        "\n",
        "            self.energyplus_runner = EnergyPlusRunner(\n",
        "                # Start EnergyPlusRunner whith the following configuration.\n",
        "                episode=self.episode,\n",
        "                env_config=self.env_config,\n",
        "                obs_queue=self.obs_queue,\n",
        "                act_queue=self.act_queue,\n",
        "                cooling_queue=self.cooling_queue,\n",
        "                heating_queue=self.heating_queue,\n",
        "                pmv_queue = self.pmv_queue,\n",
        "                ppd_queue = self.ppd_queue,\n",
        "                beta_queue = self.beta_queue,\n",
        "                emax_queue = self.emax_queue\n",
        "            )\n",
        "\n",
        "            self.energyplus_runner.start()\n",
        "            # Divide the thread in two in this point.\n",
        "            self.energyplus_runner.obs_event.wait()\n",
        "            # Wait untill an observation is made.\n",
        "            obs = self.obs_queue.get()\n",
        "            # Get the observation.\n",
        "            self.energyplus_runner.cooling_event.wait()\n",
        "            # Wait untill an cooling metric read is made.\n",
        "            ec = self.cooling_queue.get()\n",
        "            # Get the cooling metric read. It is not used here, but yes in the step method and it is necesary\n",
        "            # to liberate the space in teh queue.\n",
        "            self.energyplus_runner.heating_event.wait()\n",
        "            # Wait untill an heating metric read is made.\n",
        "            eh = self.heating_queue.get()\n",
        "            # Get the heating metric read. It is not used here, but yes in the step method and it is necesary\n",
        "            # to liberate the space in teh queue.\n",
        "            self.energyplus_runner.pmv_event.wait()\n",
        "            # Wait untill an PMV metric read is made.\n",
        "            pmv = self.pmv_queue.get()\n",
        "            # Get the PMV metric read. It is not used here, but yes in the step method and it is necesary\n",
        "            # to liberate the space in teh queue.\n",
        "            self.energyplus_runner.ppd_event.wait()\n",
        "            # Wait untill an PPD metric read is made.\n",
        "            ppd = self.ppd_queue.get()\n",
        "            # Get the PPD metric read. It is not used here, but yes in the step method and it is necesary\n",
        "            # to liberate the space in teh queue.\n",
        "\n",
        "            self.energyplus_runner.beta_event.wait()\n",
        "            # Wait untill an beta value read is made.\n",
        "            self.beta = self.beta_queue.get()\n",
        "            # Get the beta value read. It is not used here, but yes in the step method and it is necesary\n",
        "            # to liberate the space in teh queue.\n",
        "            self.energyplus_runner.emax_event.wait()\n",
        "            # Wait untill an E_max value read is made.\n",
        "            self.emax = self.emax_queue.get()\n",
        "            # Get the E_max value read. It is not used here, but yes in the step method and it is necesary\n",
        "            # to liberate the space in teh queue.\n",
        "\n",
        "            self.last_obs = obs\n",
        "            # Save the observation as a last observation.\n",
        "            self.last_beta = self.beta\n",
        "            # Save the beta as a last beta.\n",
        "            self.last_emax = self.emax\n",
        "            # Save the E_max as a last E_max.\n",
        "\n",
        "        else:\n",
        "            obs = self.last_obs\n",
        "            self.truncate_flag = False\n",
        "\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        self.timestep += 1\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        # terminated variable is used to determine the end of a episode. Is stablished as False until the\n",
        "        # environment present a terminal state.\n",
        "        timeout = 40\n",
        "        # timeout is set to 4s to handle end of simulation cases, which happens async\n",
        "        # and materializes by worker thread waiting on this queue (EnergyPlus callback\n",
        "        # not consuming yet/anymore).timeout value can be increased if E+ timestep takes longer.\n",
        "        if self.energyplus_runner.simulation_complete:\n",
        "            # simulation_complete is likely to happen after last env step()\n",
        "            # is called, hence leading to waiting on queue for a timeout.\n",
        "            if self.energyplus_runner.failed():\n",
        "                # check for simulation errors.\n",
        "                raise Exception(\"Faulty episode\")\n",
        "            terminated = True\n",
        "            # if the simulation is complete, the episode is ended.\n",
        "            obs = self.last_obs\n",
        "            # we use the last observation as a observation for the timestep.\n",
        "            self.beta = self.last_beta\n",
        "            # we use the last beta as a beta for the timestep.\n",
        "            self.emax = self.last_emax\n",
        "            # we use the last E_max as a E_max for the timestep.\n",
        "        else:\n",
        "            # if the simulation is not complete, enqueue action (received by EnergyPlus through\n",
        "            # dedicated callback) and then wait to get next observation.\n",
        "            try:\n",
        "                self.act_queue.put(action,timeout=timeout)\n",
        "                self.energyplus_runner.act_event.set()\n",
        "                # Send the action to the EnergyPlus Runner flow.\n",
        "                self.energyplus_runner.obs_event.wait(timeout=timeout)\n",
        "                obs = self.obs_queue.get(timeout=timeout)\n",
        "                # Get the return observation after the action is applied.\n",
        "                self.last_obs = obs\n",
        "                # Upgrade last observation.\n",
        "                self.beta = self.beta_queue.get(timeout=timeout)\n",
        "                self.last_beta = self.beta\n",
        "                # Get the return beta after the action is applied and upgrade last beta.\n",
        "                self.emax = self.emax_queue.get(timeout=timeout)\n",
        "                self.last_emax = self.emax\n",
        "                # Get the return E_max after the action is applied and upgrade last E_max.\n",
        "            except (Full, Empty):\n",
        "                terminated = True\n",
        "                # Set the terminated variable into True to finish the episode.\n",
        "                obs = self.last_obs\n",
        "                # We use the last observation as a observation for the timestep.\n",
        "                self.beta = self.last_beta\n",
        "                # We use the last beta as a beta for the timestep.\n",
        "                self.emax = self.last_emax\n",
        "                # We use the last E_max as a E_max for the timestep.\n",
        "\n",
        "        if self.energyplus_runner.failed():\n",
        "            # Raise an exception if the episode is faulty.\n",
        "            truncated = True\n",
        "            raise Exception(\"Faulty episode\")\n",
        "\n",
        "        self.energyplus_runner.cooling_event.wait(10)\n",
        "        if self.energyplus_runner.failed():\n",
        "            raise Exception(\"Faulty episode\")\n",
        "        self.ec = self.cooling_queue.get()\n",
        "        self.ec = (abs(self.ec))/(3600000)\n",
        "        # Wait for the cooling energy consume in the timestep\n",
        "        self.energyplus_runner.heating_event.wait(10)\n",
        "        if self.energyplus_runner.failed():\n",
        "            raise Exception(\"Faulty episode\")\n",
        "        self.eh = self.heating_queue.get()\n",
        "        self.eh = (abs(self.eh))/(3600000)\n",
        "        # Wait for the heating energy consume in the timestep\n",
        "        self.energyplus_runner.pmv_event.wait(10)\n",
        "        if self.energyplus_runner.failed():\n",
        "            raise Exception(\"Faulty episode\")\n",
        "        self.pmv = self.pmv_queue.get()\n",
        "        # Wait for the pmv factor in the timestep\n",
        "        self.energyplus_runner.ppd_event.wait(10)\n",
        "        if self.energyplus_runner.failed():\n",
        "            raise Exception(\"Faulty episode\")\n",
        "        self.ppd = self.ppd_queue.get()\n",
        "        # Wait for the ppd factor in the timestep\n",
        "\n",
        "        reward = (-self.beta*(self.eh + self.ec)/(self.emax) - (1-self.beta)*(self.ppd/100))\n",
        "        # Compute reward, energy, comfort and ppd.\n",
        "        infos = {\n",
        "            'energy': self.eh + self.ec,\n",
        "            'comfort': self.pmv,\n",
        "            'ppd': self.ppd,\n",
        "        }\n",
        "        # Save energy, comfort (pmv) and ppd in the info dictionary, used after for analisys\n",
        "\n",
        "        #truncated = self.timestep_cut(6*24*5, terminated)\n",
        "\n",
        "        return obs, reward, terminated, truncated, infos\n",
        "\n",
        "    def close(self):\n",
        "        if self.energyplus_runner is not None:\n",
        "            self.energyplus_runner.stop()\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        pass\n",
        "\n",
        "    \"\"\"def timestep_cut(self, num_timestep: int, terminated: bool):\n",
        "        if self.timestep >= num_timestep and not terminated:\n",
        "            self.truncate_flag = True\n",
        "            return True\n",
        "        else:\n",
        "            self.truncate_flag = False\n",
        "            return False\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se establece la configuración del entorno."
      ],
      "metadata": {
        "id": "lBBEpHSwKgU7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQKu-sElSPcW"
      },
      "outputs": [],
      "source": [
        "\"\"\"## DEFINE THE EXPERIMENT CONTROLS\n",
        "\"\"\"\n",
        "algorithm = 'DQN'\n",
        "# Define the algorithm to use to train the policy. Options are: PPO, SAC, DQN.\n",
        "tune_runner  = True\n",
        "# Define if the experiment tuning the variables or execute a unique configuration.\n",
        "restore = False\n",
        "# To define if is necesary to restore or not a previous experiment. Is necesary to stablish a 'restore_path'.\n",
        "restore_path = ''\n",
        "# Path to the folder where the experiment is located.\n",
        "\n",
        "env_config = {\n",
        "    'weather_folder': '/content/drive/My Drive/ep_drive/epw',\n",
        "    'output': '/content/drive/My Drive/ep_drive/output',\n",
        "    'epjson_folderpath': '/content/drive/My Drive/ep_drive/epjson',\n",
        "    'epjson_output_folder': '/content/drive/My Drive/ep_drive/models',\n",
        "    # Configure the directories for the experiment.\n",
        "    'ep_terminal_output': False,\n",
        "    # For dubugging is better to print in the terminal the outputs of the EnergyPlus simulation process.\n",
        "    'beta': 0.5,\n",
        "    # This parameter is used to balance between energy and comfort of the inhabitatns. A\n",
        "    # value equal to 0 give a no importance to comfort and a value equal to 1 give no importance\n",
        "    # to energy consume. Mathematically is the reward:\n",
        "    # r = - beta*normaliced_energy - (1-beta)*normalized_comfort\n",
        "    # The range of this value goes from 0.0 to 1.0.,\n",
        "    'is_test': False,\n",
        "    # For evaluation process 'is_test=True' and for trainig False.\n",
        "    'test_init_day': 1,\n",
        "    'action_space': gym.spaces.Discrete(4),\n",
        "    # action space for simple agent case\n",
        "    'observation_space': gym.spaces.Box(float(\"-inf\"), float(\"inf\"), (1465,)),\n",
        "    # observation space for simple agent case\n",
        "\n",
        "    # BUILDING CONFIGURATION\n",
        "    'building_name': 'prot_1',\n",
        "    'volumen': 131.6565,\n",
        "    'window_area_relation_north': 0,\n",
        "    'window_area_relation_west': 0,\n",
        "    'window_area_relation_south': 0.0115243076,\n",
        "    'window_area_relation_east': 0.0276970753,\n",
        "    'episode_len': 365,\n",
        "    'rotation': 0,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4.1.** Comprobación del seriabilidad del entorno"
      ],
      "metadata": {
        "id": "nFFVrTlKKpFn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yvpdvUZG6lM",
        "outputId": "3693eece-d148-452e-d320-fcd967469aeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===============================================================\n",
            "Checking Serializability of <class '__main__.EnergyPlusEnv_v0'>\n",
            "===============================================================\n",
            "!!! FAIL serialization: ctypes objects containing pointers cannot be pickled\n",
            "    Serializing '__enter__' <function Env.__enter__ at 0x7e12871af520>...\n",
            "    Serializing '__exit__' <function Env.__exit__ at 0x7e12871af5b0>...\n",
            "    Serializing '__init__' <function EnergyPlusEnv_v0.__init__ at 0x7e1287a75900>...\n",
            "    !!! FAIL serialization: ctypes objects containing pointers cannot be pickled\n",
            "    Detected 1 global variables. Checking serializability...\n",
            "        Serializing 'env_config' {'weather_folder': '/content/drive/My Drive/ep_drive/epw', 'output': '/content/drive/My Drive/ep_drive/output', 'epjson_folderpath': '/content/drive/My Drive/ep_drive/epjson', 'epjson_output_folder': '/content/drive/My Drive/ep_drive/models', 'ep_terminal_output': False, 'beta': 0.5, 'is_test': False, 'test_init_day': 1, 'action_space': Discrete(4), 'observation_space': Box(-inf, inf, (1465,), float32), 'building_name': 'prot_1', 'volumen': 131.6565, 'window_area_relation_north': 0, 'window_area_relation_west': 0, 'window_area_relation_south': 0.0115243076, 'window_area_relation_east': 0.0276970753, 'episode_len': 365, 'rotation': 0}...\n",
            "    Detected 1 nonlocal variables. Checking serializability...\n",
            "        Serializing '__class__' <class '__main__.EnergyPlusEnv_v0'>...\n",
            "        !!! FAIL serialization: ctypes objects containing pointers cannot be pickled\n",
            "            Serializing '__enter__' <function Env.__enter__ at 0x7e12871af520>...\n",
            "            Serializing '__exit__' <function Env.__exit__ at 0x7e12871af5b0>...\n",
            "            Serializing '__init__' <function EnergyPlusEnv_v0.__init__ at 0x7e1287a75900>...\n",
            "            !!! FAIL serialization: ctypes objects containing pointers cannot be pickled\n",
            "            Detected 1 global variables. Checking serializability...\n",
            "                Serializing 'env_config' {'weather_folder': '/content/drive/My Drive/ep_drive/epw', 'output': '/content/drive/My Drive/ep_drive/output', 'epjson_folderpath': '/content/drive/My Drive/ep_drive/epjson', 'epjson_output_folder': '/content/drive/My Drive/ep_drive/models', 'ep_terminal_output': False, 'beta': 0.5, 'is_test': False, 'test_init_day': 1, 'action_space': Discrete(4), 'observation_space': Box(-inf, inf, (1465,), float32), 'building_name': 'prot_1', 'volumen': 131.6565, 'window_area_relation_north': 0, 'window_area_relation_west': 0, 'window_area_relation_south': 0.0115243076, 'window_area_relation_east': 0.0276970753, 'episode_len': 365, 'rotation': 0}...\n",
            "            Detected 1 nonlocal variables. Checking serializability...\n",
            "                Serializing '__class__' <class '__main__.EnergyPlusEnv_v0'>...\n",
            "                !!! FAIL serialization: ctypes objects containing pointers cannot be pickled\n",
            "                    Serializing '__enter__' <function Env.__enter__ at 0x7e12871af520>...\n",
            "                    Serializing '__exit__' <function Env.__exit__ at 0x7e12871af5b0>...\n",
            "                    Serializing '__init__' <function EnergyPlusEnv_v0.__init__ at 0x7e1287a75900>...\n",
            "                    !!! FAIL serialization: ctypes objects containing pointers cannot be pickled\n",
            "                    Detected 1 global variables. Checking serializability...\n",
            "                        Serializing 'env_config' {'weather_folder': '/content/drive/My Drive/ep_drive/epw', 'output': '/content/drive/My Drive/ep_drive/output', 'epjson_folderpath': '/content/drive/My Drive/ep_drive/epjson', 'epjson_output_folder': '/content/drive/My Drive/ep_drive/models', 'ep_terminal_output': False, 'beta': 0.5, 'is_test': False, 'test_init_day': 1, 'action_space': Discrete(4), 'observation_space': Box(-inf, inf, (1465,), float32), 'building_name': 'prot_1', 'volumen': 131.6565, 'window_area_relation_north': 0, 'window_area_relation_west': 0, 'window_area_relation_south': 0.0115243076, 'window_area_relation_east': 0.0276970753, 'episode_len': 365, 'rotation': 0}...\n",
            "                    Detected 1 nonlocal variables. Checking serializability...\n",
            "                        Serializing '__class__' <class '__main__.EnergyPlusEnv_v0'>...\n",
            "                        !!! FAIL serialization: ctypes objects containing pointers cannot be pickled\n",
            "                            Serializing '__enter__' <function Env.__enter__ at 0x7e12871af520>...\n",
            "                            Serializing '__exit__' <function Env.__exit__ at 0x7e12871af5b0>...\n",
            "                            Serializing '__init__' <function EnergyPlusEnv_v0.__init__ at 0x7e1287a75900>...\n",
            "                            !!! FAIL serialization: ctypes objects containing pointers cannot be pickled\n",
            "                            Detected 1 global variables. Checking serializability...\n",
            "                                Serializing 'env_config' {'weather_folder': '/content/drive/My Drive/ep_drive/epw', 'output': '/content/drive/My Drive/ep_drive/output', 'epjson_folderpath': '/content/drive/My Drive/ep_drive/epjson', 'epjson_output_folder': '/content/drive/My Drive/ep_drive/models', 'ep_terminal_output': False, 'beta': 0.5, 'is_test': False, 'test_init_day': 1, 'action_space': Discrete(4), 'observation_space': Box(-inf, inf, (1465,), float32), 'building_name': 'prot_1', 'volumen': 131.6565, 'window_area_relation_north': 0, 'window_area_relation_west': 0, 'window_area_relation_south': 0.0115243076, 'window_area_relation_east': 0.0276970753, 'episode_len': 365, 'rotation': 0}...\n",
            "                            Detected 1 nonlocal variables. Checking serializability...\n",
            "                                Serializing '__class__' <class '__main__.EnergyPlusEnv_v0'>...\n",
            "                                !!! FAIL serialization: ctypes objects containing pointers cannot be pickled\n",
            "                                    Serializing '__enter__' <function Env.__enter__ at 0x7e12871af520>...\n",
            "                                    Serializing '__exit__' <function Env.__exit__ at 0x7e12871af5b0>...\n",
            "                                    Serializing '__init__' <function EnergyPlusEnv_v0.__init__ at 0x7e1287a75900>...\n",
            "                                    !!! FAIL serialization: ctypes objects containing pointers cannot be pickled\n",
            "                                    Detected 1 global variables. Checking serializability...\n",
            "                                        Serializing 'env_config' {'weather_folder': '/content/drive/My Drive/ep_drive/epw', 'output': '/content/drive/My Drive/ep_drive/output', 'epjson_folderpath': '/content/drive/My Drive/ep_drive/epjson', 'epjson_output_folder': '/content/drive/My Drive/ep_drive/models', 'ep_terminal_output': False, 'beta': 0.5, 'is_test': False, 'test_init_day': 1, 'action_space': Discrete(4), 'observation_space': Box(-inf, inf, (1465,), float32), 'building_name': 'prot_1', 'volumen': 131.6565, 'window_area_relation_north': 0, 'window_area_relation_west': 0, 'window_area_relation_south': 0.0115243076, 'window_area_relation_east': 0.0276970753, 'episode_len': 365, 'rotation': 0}...\n",
            "                                    Detected 1 nonlocal variables. Checking serializability...\n",
            "                                        Serializing '__class__' <class '__main__.EnergyPlusEnv_v0'>...\n",
            "                                        !!! FAIL serialization: ctypes objects containing pointers cannot be pickled\n",
            "                                    Serializing '_is_protocol' False...\n",
            "                            Serializing '_is_protocol' False...\n",
            "                    Serializing '_is_protocol' False...\n",
            "            Serializing '_is_protocol' False...\n",
            "    Serializing '_is_protocol' False...\n",
            "===============================================================\n",
            "Variable: \n",
            "\n",
            "\tFailTuple(__class__ [obj=<class '__main__.EnergyPlusEnv_v0'>, parent=<function EnergyPlusEnv_v0.__init__ at 0x7e1287a75900>])\n",
            "\n",
            "was found to be non-serializable. There may be multiple other undetected variables that were non-serializable. \n",
            "Consider either removing the instantiation/imports of these variables or moving the instantiation into the scope of the function/class. \n",
            "===============================================================\n",
            "Check https://docs.ray.io/en/master/ray-core/objects/serialization.html#troubleshooting for more information.\n",
            "If you have any suggestions on how to improve this error message, please reach out to the Ray developers on github.com/ray-project/ray/issues/\n",
            "===============================================================\n",
            "Is serializable: False\n",
            "Unserializable objects:\n",
            "FailTuple(__class__ [obj=<class '__main__.EnergyPlusEnv_v0'>, parent=<function EnergyPlusEnv_v0.__init__ at 0x7e1287a75900>])\n"
          ]
        }
      ],
      "source": [
        "from ray.util import inspect_serializability\n",
        "\n",
        "# Assume `env` is your environment\n",
        "is_serializable, unserializable_objects = inspect_serializability(EnergyPlusEnv_v0, depth=10)\n",
        "print(f\"Is serializable: {is_serializable}\")\n",
        "if not is_serializable:\n",
        "    print(\"Unserializable objects:\")\n",
        "    for obj in unserializable_objects:\n",
        "        print(obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HxWrmtjVw2s"
      },
      "source": [
        "## **5**. Confuguración del algoritmo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "FXmLY2GeWhfK",
        "outputId": "01289263-5cd7-4e3a-bae0-6812d586f9ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-16 10:33:32,115\tINFO worker.py:1715 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ctypes objects containing pointers cannot be pickled",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-b71ce5108280>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Inicialiced Ray Server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mregister_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"EPEnv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEnergyPlusEnv_v0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Register the environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/registry.py\u001b[0m in \u001b[0;36mregister_env\u001b[0;34m(name, env_creator)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Second argument must be callable.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0m_global_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENV_CREATOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/registry.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, category, key, value)\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0;34m\"Unknown category {} not among {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNOWN_CATEGORIES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             )\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_flush\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_internal_kv_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/cloudpickle/__init__.py\u001b[0m in \u001b[0;36mdumps_debug\u001b[0;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdumps_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPicklingError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RAY_PICKLE_VERBOSE_DEBUG\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"recursion\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: ctypes objects containing pointers cannot be pickled"
          ]
        }
      ],
      "source": [
        "ray.init()\n",
        "# Inicialiced Ray Server\n",
        "register_env(name=\"EPEnv\", env_creator=lambda args: EnergyPlusEnv_v0(args))\n",
        "# Register the environment.\n",
        "\n",
        "def trial_str_creator(trial):\n",
        "    return \"{}_{}_{}_REP\".format(trial.trainable_name, trial.trial_id, tune.search.repeater.TRIAL_INDEX)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrcARdTvWKBY"
      },
      "source": [
        "### **5.1.** PPO Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouBr6RkZd5fP"
      },
      "outputs": [],
      "source": [
        "algorithm = 'PPO'\n",
        "# PPO Algorithm Config\n",
        "algo = PPOConfig().training(\n",
        "  # General Algo Configs\n",
        "  gamma=0.72 if not tune_runner else tune.uniform(0.7, 0.99),\n",
        "  # Float specifying the discount factor of the Markov Decision process.\n",
        "  lr=0.04 if not tune_runner else tune.uniform(0.001, 0.1),\n",
        "  # The learning rate (float) or learning rate schedule\n",
        "  #model=,\n",
        "  # Arguments passed into the policy model. See models/catalog.py for a full list of the\n",
        "  # available model options.\n",
        "  train_batch_size=128,# if not tune_runner else tune.choice([128, 256]),\n",
        "  # PPO Configs\n",
        "  lr_schedule=None, # List[List[int | float]] | None = NotProvided,\n",
        "  # Learning rate schedule. In the format of [[timestep, lr-value], [timestep, lr-value], …]\n",
        "  # Intermediary timesteps will be assigned to interpolated learning rate values. A schedule\n",
        "  # should normally start from timestep 0.\n",
        "  use_critic=True, # bool | None = NotProvided,\n",
        "  # Should use a critic as a baseline (otherwise don’t use value baseline; required for using GAE).\n",
        "  use_gae=True, # bool | None = NotProvided,\n",
        "  # If true, use the Generalized Advantage Estimator (GAE) with a value function,\n",
        "  # see https://arxiv.org/pdf/1506.02438.pdf.\n",
        "  lambda_=0.20216 if not tune_runner else tune.uniform(0, 1.0), # float | None = NotProvided,\n",
        "  # The GAE (lambda) parameter.  The generalized advantage estimator for 0 < λ < 1 makes a\n",
        "  # compromise between bias and variance, controlled by parameter λ.\n",
        "  use_kl_loss=True, # bool | None = NotProvided,\n",
        "  # Whether to use the KL-term in the loss function.\n",
        "  kl_coeff=9.9712 if not tune_runner else tune.uniform(0.3, 10.0), # float | None = NotProvided,\n",
        "  # Initial coefficient for KL divergence.\n",
        "  kl_target=0.054921 if not tune_runner else tune.uniform(0.001, 0.1), # float | None = NotProvided,\n",
        "  # Target value for KL divergence.\n",
        "  sgd_minibatch_size=48,# if not tune_runner else tune.choice([48, 128]), # int | None = NotProvided,\n",
        "  # Total SGD batch size across all devices for SGD. This defines the minibatch size\n",
        "  # within each epoch.\n",
        "  num_sgd_iter=6,# if not tune_runner else tune.randint(30, 60), # int | None = NotProvided,\n",
        "  # Number of SGD iterations in each outer loop (i.e., number of epochs to execute per train batch).\n",
        "  shuffle_sequences=True, # bool | None = NotProvided,\n",
        "  # Whether to shuffle sequences in the batch when training (recommended).\n",
        "  vf_loss_coeff=0.38584 if not tune_runner else tune.uniform(0.1, 1.0), # Tune this! float | None = NotProvided,\n",
        "  # Coefficient of the value function loss. IMPORTANT: you must tune this if you set\n",
        "  # vf_share_layers=True inside your model’s config.\n",
        "  entropy_coeff=10.319 if not tune_runner else tune.uniform(0.95, 15.0), # float | None = NotProvided,\n",
        "  # Coefficient of the entropy regularizer.\n",
        "  entropy_coeff_schedule=None, # List[List[int | float]] | None = NotProvided,\n",
        "  # Decay schedule for the entropy regularizer.\n",
        "  clip_param=0.22107 if not tune_runner else tune.uniform(0.1, 0.4), # float | None = NotProvided,\n",
        "  # The PPO clip parameter.\n",
        "  vf_clip_param=39.327 if not tune_runner else tune.uniform(0, 50), # float | None = NotProvided,\n",
        "  # Clip param for the value function. Note that this is sensitive to the scale of the\n",
        "  # rewards. If your expected V is large, increase this.\n",
        "  grad_clip=None, # float | None = NotProvided,\n",
        "  # If specified, clip the global norm of gradients by this amount.\n",
        ").environment(\n",
        "  env=\"EPEnv\",\n",
        "  observation_space=gym.spaces.Box(float(\"-inf\"), float(\"inf\"), (49,)),\n",
        "  action_space=gym.spaces.Discrete(4),\n",
        "  env_config=env_config,\n",
        ").framework(\n",
        "  framework = 'torch',\n",
        ").fault_tolerance(\n",
        "  recreate_failed_workers = True,\n",
        "  restart_failed_sub_environments=False,\n",
        ").rollouts(\n",
        "  num_rollout_workers = 1,# if not tune_runner else tune.grid_search([0, 1, 3]),\n",
        "  create_env_on_local_worker=True,\n",
        "  rollout_fragment_length = 'auto',\n",
        "  enable_connectors = True,\n",
        "  #batch_mode=\"truncate_episodes\",\n",
        "  num_envs_per_worker=1,\n",
        ").experimental(\n",
        "  _enable_new_api_stack = True,\n",
        ").reporting( # multi_agent config va aquí\n",
        "  min_sample_timesteps_per_iteration = 2000,\n",
        ").checkpointing(\n",
        "  export_native_model_files = True,\n",
        ").debugging(\n",
        "  log_level = \"ERROR\",\n",
        "  #seed=7,# if not tune_runner else tune.grid_search([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
        ").resources(\n",
        "  num_gpus = 0,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NyWAXdQXmE8"
      },
      "source": [
        "### **5.2.** DQN Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj0j3WmhWYLT"
      },
      "outputs": [],
      "source": [
        "algo = DQNConfig().training(\n",
        "  # General Algo Configs\n",
        "  gamma = 0.7 if not tune_runner else tune.uniform(0.7, 0.99),\n",
        "  lr = 0.1 if not tune_runner else tune.uniform(0.001, 0.3),\n",
        "  grad_clip = 0.5 if not tune_runner else tune.uniform(0.5, 40.0),\n",
        "  grad_clip_by = 'global_norm',\n",
        "  train_batch_size = 8,# if not tune_runner else tune.choice([4, 8, 128, 256]),\n",
        "  model = {\n",
        "    \"fcnet_hiddens\": [1024,512,512,512],\n",
        "    \"fcnet_activation\": \"relu\", #if not tune_runner else tune.choice(['tanh', 'relu', 'swish', 'linear']),\n",
        "  },\n",
        "  optimizer = {},\n",
        "  # DQN Configs\n",
        "  num_atoms = 40,\n",
        "  v_min = -1,\n",
        "  v_max = 0,\n",
        "  noisy = True,\n",
        "  sigma0 = 0.66 if not tune_runner else tune.uniform(0, 1),\n",
        "  dueling = True,\n",
        "  hiddens = [512],\n",
        "  double_q = True,\n",
        "  n_step = 24,\n",
        "  replay_buffer_config = {\n",
        "    '_enable_replay_buffer_api': True,\n",
        "    'type': 'MultiAgentPrioritizedReplayBuffer',\n",
        "    'capacity': 100000,\n",
        "    'prioritized_replay_alpha': 0.6,\n",
        "    'prioritized_replay_beta': 0.4,\n",
        "    'prioritized_replay_eps': 1e-6,\n",
        "    'replay_sequence_length': 1,\n",
        "  },\n",
        "  categorical_distribution_temperature = 0.5 if not tune_runner else tune.uniform(0, 1),\n",
        ").environment(\n",
        "  env=\"EPEnv\",\n",
        "  env_config=env_config,\n",
        ").framework(\n",
        "  framework = 'torch',\n",
        ").fault_tolerance(\n",
        "  recreate_failed_workers = True,\n",
        "  restart_failed_sub_environments=False,\n",
        ").rollouts(\n",
        "  num_rollout_workers = 1,\n",
        "  create_env_on_local_worker=True,\n",
        "  rollout_fragment_length = 'auto',\n",
        "  enable_connectors = True,\n",
        "  num_envs_per_worker=1,\n",
        ").experimental(\n",
        "  _enable_new_api_stack = False,\n",
        ").reporting( # multi_agent config va aquí\n",
        "  min_sample_timesteps_per_iteration = 1000,\n",
        ").checkpointing(\n",
        "  export_native_model_files = True,\n",
        ").debugging(\n",
        "  log_level = \"ERROR\",\n",
        "  #seed=7,\n",
        ").resources(\n",
        "  num_gpus = 0,\n",
        ")\n",
        "algo.exploration(\n",
        "  exploration_config={\n",
        "    \"type\": \"EpsilonGreedy\",\n",
        "    \"initial_epsilon\": 1.,\n",
        "    \"final_epsilon\": 0.,\n",
        "    \"epsilon_timesteps\": 6*24*365*10,\n",
        "  }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te9UG0-NYNld"
      },
      "source": [
        "### **5.3.** SAC Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5ZcaV8DWa2k"
      },
      "outputs": [],
      "source": [
        "algorithm = 'SAC'\n",
        "algo = SACConfig().training(\n",
        "  # General Algo Configs\n",
        "  gamma = 0.99 if not tune_runner else tune.uniform(0.7, 0.99),\n",
        "  # Float specifying the discount factor of the Markov Decision process.\n",
        "  lr = 0.1 if not tune_runner else tune.uniform(0.001, 0.1),\n",
        "  # The learning rate (float) or learning rate schedule\n",
        "  #grad_clip = None, #float\n",
        "  # If None, no gradient clipping will be applied. Otherwise, depending on the setting of grad_clip_by, the (float)\n",
        "  # value of grad_clip will have the following effect: If grad_clip_by=value: Will clip all computed gradients\n",
        "  # individually inside the interval [-grad_clip, +`grad_clip`]. If grad_clip_by=norm, will compute the L2-norm of\n",
        "  # each weight/bias gradient tensor individually and then clip all gradients such that these L2-norms do not exceed\n",
        "  # grad_clip. The L2-norm of a tensor is computed via: sqrt(SUM(w0^2, w1^2, ..., wn^2)) where w[i] are the elements\n",
        "  # of the tensor (no matter what the shape of this tensor is). If grad_clip_by=global_norm, will compute the square\n",
        "  # of the L2-norm of each weight/bias gradient tensor individually, sum up all these squared L2-norms across all\n",
        "  # given gradient tensors (e.g. the entire module to be updated), square root that overall sum, and then clip all\n",
        "  # gradients such that this global L2-norm does not exceed the given value. The global L2-norm over a list of tensors\n",
        "  # (e.g. W and V) is computed via: sqrt[SUM(w0^2, w1^2, ..., wn^2) + SUM(v0^2, v1^2, ..., vm^2)], where w[i] and v[j]\n",
        "  # are the elements of the tensors W and V (no matter what the shapes of these tensors are).\n",
        "  #grad_clip_by = 'global_norm', #str\n",
        "  # See grad_clip for the effect of this setting on gradient clipping. Allowed values are value, norm, and global_norm.\n",
        "  #train_batch_size = 128, # if not tune_runner else tune.randint(128, 257),\n",
        "  #  Training batch size, if applicable.\n",
        "  model = {\n",
        "    \"fcnet_hiddens\": [256],\n",
        "    \"fcnet_activation\": \"relu\",\n",
        "  },\n",
        "  # Arguments passed into the policy model. See models/catalog.py for a full list of the\n",
        "  # available model options. TODO: Provide ModelConfig objects instead of dicts\n",
        "  #optimizer = None, #dict\n",
        "  # Arguments to pass to the policy optimizer. This setting is not used when _enable_new_api_stack=True.\n",
        "  #max_requests_in_flight_per_sampler_worker = None, #int\n",
        "  # Max number of inflight requests to each sampling worker. See the FaultTolerantActorManager class for more details.\n",
        "  # Tuning these values is important when running experimens with large sample batches, where there is the risk that\n",
        "  # the object store may fill up, causing spilling of objects to disk. This can cause any asynchronous requests to\n",
        "  # become very slow, making your experiment run slow as well. You can inspect the object store during your experiment\n",
        "  # via a call to ray memory on your headnode, and by using the ray dashboard. If you’re seeing that the object store\n",
        "  # is filling up, turn down the number of remote requests in flight, or enable compression in your experiment of\n",
        "  # timesteps.\n",
        "  #learner_class = None,\n",
        "  # The Learner class to use for (distributed) updating of the RLModule. Only used when _enable_new_api_stack=True.\n",
        "\n",
        "  # SAC Configs\n",
        "  twin_q = True, #bool\n",
        "  # Use two Q-networks (instead of one) for action-value estimation. Note: Each Q-network will have its own target network.\n",
        "  #q_model_config = #~typing.Dict[str, ~typing.Any]\n",
        "  # Model configs for the Q network(s). These will override MODEL_DEFAULTS. This is treated just as the top-level model\n",
        "  # dict in setting up the Q-network(s) (2 if twin_q=True). That means, you can do for different observation spaces:\n",
        "  # obs=Box(1D) -> Tuple(Box(1D) + Action) -> concat -> post_fcnet obs=Box(3D) -> Tuple(Box(3D) + Action) ->\n",
        "  # vision-net -> concat w/ action -> post_fcnet obs=Tuple(Box(1D), Box(3D)) -> Tuple(Box(1D), Box(3D), Action) ->\n",
        "  # vision-net -> concat w/ Box(1D) and action -> post_fcnet You can also have SAC use your custom_model as Q-model(s),\n",
        "  # by simply specifying the custom_model sub-key in below dict (just like you would do in the top-level model dict.\n",
        "  #policy_model_config = #~typing.Dict[str, ~typing.Any]\n",
        "  # Model options for the policy function (see q_model_config above for details). The difference to q_model_config above\n",
        "  # is that no action concat’ing is performed before the post_fcnet stack.\n",
        "  tau = 1.0, #float\n",
        "  # Update the target by au * policy + (1- au) * target_policy.\n",
        "  initial_alpha = 0.5, #float\n",
        "  # Initial value to use for the entropy weight alpha.\n",
        "  target_entropy = 'auto', #str | float\n",
        "  # Target entropy lower bound. If “auto”, will be set to -|A| (e.g. -2.0 for Discrete(2), -3.0 for Box(shape=(3,))). This\n",
        "  # is the inverse of reward scale, and will be optimized automatically.\n",
        "  n_step = 10, # if not tune_runner else tune.randint(1, 11), #int\n",
        "  # N-step target updates. If >1, sars’ tuples in trajectories will be postprocessed to become\n",
        "  # sa[discounted sum of R][s t+n] tuples.\n",
        "  store_buffer_in_checkpoints = True, #bool\n",
        "  # Set this to True, if you want the contents of your buffer(s) to be stored in any saved checkpoints as well. Warnings\n",
        "  # will be created if: - This is True AND restoring from a checkpoint that contains no buffer data. - This is\n",
        "  # False AND restoring from a checkpoint that does contain buffer data.\n",
        "  replay_buffer_config = {\n",
        "    '_enable_replay_buffer_api': True,\n",
        "    'type': 'MultiAgentPrioritizedReplayBuffer',\n",
        "    'capacity': 50000,\n",
        "    'prioritized_replay_alpha': 0.6,\n",
        "    'prioritized_replay_beta': 0.4,\n",
        "    'prioritized_replay_eps': 1e-6,\n",
        "    'replay_sequence_length': 1,\n",
        "  },\n",
        "  # Replay buffer config. Examples: { “_enable_replay_buffer_api”: True, “type”: “MultiAgentReplayBuffer”,\n",
        "  # “capacity”: 50000, “replay_batch_size”: 32, “replay_sequence_length”: 1, } - OR - { “_enable_replay_buffer_api”: True,\n",
        "  # “type”: “MultiAgentPrioritizedReplayBuffer”, “capacity”: 50000, “prioritized_replay_alpha”: 0.6,\n",
        "  # “prioritized_replay_beta”: 0.4, “prioritized_replay_eps”: 1e-6, “replay_sequence_length”: 1, } - Where -\n",
        "  # prioritized_replay_alpha: Alpha parameter controls the degree of prioritization in the buffer. In other words, when\n",
        "  # a buffer sample has a higher temporal-difference error, with how much more probability should it drawn to use\n",
        "  # to update the parametrized Q-network. 0.0 corresponds to uniform probability. Setting much above 1.0 may quickly\n",
        "  # result as the sampling distribution could become heavily “pointy” with low entropy. prioritized_replay_beta: Beta\n",
        "  # parameter controls the degree of importance sampling which suppresses the influence of gradient updates from\n",
        "  # samples that have higher probability of being sampled via alpha parameter and the temporal-difference error.\n",
        "  # prioritized_replay_eps: Epsilon parameter sets the baseline probability for sampling so that when the\n",
        "  # temporal-difference error of a sample is zero, there is still a chance of drawing the sample.\n",
        "  #training_intensity = #float\n",
        "  # The intensity with which to update the model (vs collecting samples from the env). If None, uses “natural” values\n",
        "  # of: train_batch_size / (rollout_fragment_length x num_workers x num_envs_per_worker). If not None, will make sure\n",
        "  # that the ratio between timesteps inserted into and sampled from th buffer matches the given values. Example:\n",
        "  # training_intensity=1000.0 train_batch_size=250 rollout_fragment_length=1 num_workers=1 (or 0) num_envs_per_worker=1 ->\n",
        "  # natural value = 250 / 1 = 250.0 -> will make sure that replay+train op will be executed 4x asoften as rollout+insert\n",
        "  # op (4 * 250 = 1000). See: rllib/algorithms/dqn/dqn.py::calculate_rr_weights for further details.\n",
        "  clip_actions = True, #bool\n",
        "  # Whether to clip actions. If actions are already normalized, this should be set to False.\n",
        "  #grad_clip = #float\n",
        "  # If not None, clip gradients during optimization at this value.\n",
        "  optimization_config = { #~typing.Dict[str, ~typing.Any]\n",
        "    'actor_learning_rate': 0.005,\n",
        "    'critic_learning_rate': 0.005,\n",
        "    'entropy_learning_rate': 0.0001,\n",
        "  },\n",
        "  # Config dict for optimization. Set the supported keys actor_learning_rate, critic_learning_rate, and\n",
        "  # entropy_learning_rate in here.\n",
        "  target_network_update_freq = 144, #int\n",
        "  # Update the target network every target_network_update_freq steps.\n",
        "  #_deterministic_loss = #bool\n",
        "  # Whether the loss should be calculated deterministically (w/o the stochastic action sampling step). True only useful\n",
        "  # for continuous actions and for debugging.\n",
        "  #_use_beta_distribution = #bool\n",
        "  # Use a Beta-distribution instead of a SquashedGaussian for bounded, continuous action spaces (not recommended; for\n",
        "  # debugging only).\n",
        ").environment(\n",
        "  env=\"EPEnv\",\n",
        "  observation_space=gym.spaces.Box(float(\"-inf\"), float(\"inf\"), (49,)),\n",
        "  action_space=gym.spaces.Discrete(4),\n",
        "  env_config=env_config,\n",
        ").framework(\n",
        "  framework = 'torch',\n",
        ").fault_tolerance(\n",
        "  recreate_failed_workers = True,\n",
        "  restart_failed_sub_environments=False,\n",
        ").rollouts(\n",
        "  num_rollout_workers = 1,# if not tune_runner else tune.grid_search([0, 1, 3]),\n",
        "  create_env_on_local_worker=True,\n",
        "  rollout_fragment_length = 'auto',\n",
        "  enable_connectors = True,\n",
        "  #batch_mode=\"truncate_episodes\",\n",
        "  num_envs_per_worker=1,\n",
        ").experimental(\n",
        "  _enable_new_api_stack = True,\n",
        ").reporting( # multi_agent config va aquí\n",
        "  min_sample_timesteps_per_iteration = 2000,\n",
        ").checkpointing(\n",
        "  export_native_model_files = True,\n",
        ").debugging(\n",
        "  log_level = \"ERROR\",\n",
        ").resources(\n",
        "  num_gpus = 0,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W79l5StOYnMb"
      },
      "source": [
        "## **6.** Ejecución del experimento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Svox8sD4Yls-"
      },
      "outputs": [],
      "source": [
        "if not restore:\n",
        "    tune.Tuner(\n",
        "        algorithm,\n",
        "        tune_config=tune.TuneConfig(\n",
        "            mode=\"max\",\n",
        "            metric=\"episode_reward_mean\",\n",
        "            num_samples=1000,\n",
        "            # This is necesary to iterative execute the search_alg to improve the hyperparameters\n",
        "            reuse_actors=False,\n",
        "            trial_name_creator=trial_str_creator,\n",
        "            trial_dirname_creator=trial_str_creator,\n",
        "\n",
        "            #search_alg = Repeater(BayesOptSearch(),repeat=10),\n",
        "            search_alg = BayesOptSearch(),\n",
        "            # Search algorithm\n",
        "\n",
        "            #scheduler = ASHAScheduler(time_attr = 'timesteps_total', max_t=6*24*365*3, grace_period=6*24*365),\n",
        "            # Scheduler algorithm\n",
        "\n",
        "        ),\n",
        "        run_config=air.RunConfig(\n",
        "            name='BOS_VN_P1_'+str(env_config['beta'])+'_'+str(algorithm),\n",
        "            stop={\"timesteps_total\": 6*24*365*20},\n",
        "            log_to_file=True,\n",
        "\n",
        "            checkpoint_config=air.CheckpointConfig(\n",
        "                checkpoint_at_end = True,\n",
        "                checkpoint_frequency = 40,\n",
        "                #num_to_keep = 20\n",
        "            ),\n",
        "            failure_config=air.FailureConfig(\n",
        "                max_failures=100\n",
        "                # Tries to recover a run up to this many times.\n",
        "            ),\n",
        "        ),\n",
        "        param_space=algo.to_dict(),\n",
        "    ).fit()\n",
        "\n",
        "else:\n",
        "    tune.Tuner.restore(\n",
        "        path=restore_path,\n",
        "        trainable = algorithm,\n",
        "        resume_errored=True\n",
        "    )\n",
        "\n",
        "\"\"\"## END EXPERIMENT AND SHUTDOWN RAY SERVE\n",
        "\"\"\"\n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYDclTeOj6Q8"
      },
      "outputs": [],
      "source": [
        "ray.shutdown()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "YGN5pZecLFPn",
        "IvJvHYM9LL_T",
        "EXNK9i1HUF3s",
        "5GNTE9IeVYni",
        "lrcARdTvWKBY",
        "1NyWAXdQXmE8",
        "Te9UG0-NYNld"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}